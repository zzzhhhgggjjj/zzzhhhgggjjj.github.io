<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>数据中台技术</title>
      <link href="/wenzhang/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E6%8A%80%E6%9C%AF/"/>
      <url>/wenzhang/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E6%8A%80%E6%9C%AF/</url>
      
        <content type="html"><![CDATA[<div class="video-container">[up主专用，视频内嵌代码贴在这]</div><style>.video-container {    position: relative;    width: 100%;    padding-top: 56.25%; /* 16:9 aspect ratio (height/width = 9/16 * 100%) */}.video-container iframe {    position: absolute;    top: 0;    left: 0;    width: 100%;    height: 100%;}</style><!-- ---title: 数据中台技术<p>date: 1739787588000<br>tags:<br>— –&gt;</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>数据治理</title>
      <link href="/wenzhang/%E6%95%B0%E6%8D%AE%E6%B2%BB%E7%90%86/"/>
      <url>/wenzhang/%E6%95%B0%E6%8D%AE%E6%B2%BB%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h3 id="数据治理考评平台"><a href="#数据治理考评平台" class="headerlink" title="数据治理考评平台"></a>数据治理考评平台</h3><p>考评平台内部主要由两部分组成。</p><ul><li>一部分是最核心的“考评核算引擎”，主要功能是结合考评平台数据考核参数以及各个数据组件的相关信息，比如Hive元数据库、DolphinScheduler 中配置的任务信息、任务完成状况、 HDFS集群的相关信息，进行考评最终核算出分数，并进行保存。</li><li>另一部分是服务于外部访问的请求，比如查看考评、维护参数、补充元数据等等<br><img src="https://img01.zzh36111.us.kg/20250217180430.png" alt="架构图"></li></ul><h3 id="项目背景："><a href="#项目背景：" class="headerlink" title="项目背景："></a>项目背景：</h3><p>随着大数据技术的广泛应用，企业搭建了各种数据仓库、数据湖，但由于数据源复杂、口径不统一等问题，导致数据质量低、数据孤岛严重、安全风险高。因此，数据治理成为企业必不可少的环节。本项目旨在建立一套数据治理考评平台，以评估和优化企业的数据治理水平。</p><h3 id="项目目标："><a href="#项目目标：" class="headerlink" title="项目目标："></a>项目目标：</h3><p>量化数据治理效果，提供标准化评分体系<br>发现并改善数据质量、规范性、安全性等问题<br>通过自动化考评减少人工干预，提高治理效率</p><h3 id="核心功能："><a href="#核心功能：" class="headerlink" title="核心功能："></a>核心功能：</h3><ol><li>数据考评指标体系</li></ol><p>规范性（如表命名规范、字段备注）<br>存储管理（如生命周期、空表检测）<br>计算质量（如长期无产出、计算报错）<br>数据质量（如数据产出时效、数据量监控）<br>安全性（如权限管理、安全等级）</p><h1 id="数据治理考评指标"><a href="#数据治理考评指标" class="headerlink" title="数据治理考评指标"></a>数据治理考评指标</h1><p>在数据治理考评平台中，我们将考评指标分为五个大类：<strong>规范、存储、计算、质量、安全</strong>，并针对不同考核项进行打分评估。</p><h2 id="1-规范-Standardization"><a href="#1-规范-Standardization" class="headerlink" title="1. 规范 (Standardization)"></a>1. 规范 (Standardization)</h2><table><thead><tr><th>考评指标</th><th>评分标准</th><th>需要信息</th></tr></thead><tbody><tr><td>有技术负责人 (Technical Owner)</td><td>有则10分，无则0分</td><td>元数据</td></tr><tr><td>有业务负责人 (Business Owner)</td><td>有则10分，无则0分</td><td>元数据</td></tr><tr><td>表名是否合规 (Table Naming Compliance)</td><td>参考数仓表规范，不合规0分，部分合规5分，完全合规10分</td><td>元数据</td></tr><tr><td>表有备注 (Table Comment)</td><td>有则10分，无则0分</td><td>元数据</td></tr><tr><td>字段有备注信息 (Column Comment)</td><td>(备注字段数 &#x2F; 总字段数) * 10分</td><td>元数据</td></tr></tbody></table><h2 id="2-存储-Storage"><a href="#2-存储-Storage" class="headerlink" title="2. 存储 (Storage)"></a>2. 存储 (Storage)</h2><table><thead><tr><th>考评指标</th><th>评分标准</th><th>需要信息</th></tr></thead><tbody><tr><td>生命周期合理 (Lifecycle Management)</td><td>未设置生命周期0分，普通生命周期10分，超长存储5分</td><td>元数据</td></tr><tr><td>是否为空表 (Empty Table)</td><td>空表0分，有数据10分</td><td>元数据</td></tr><tr><td>存在相似表 (Duplicate Tables)</td><td>相似表字段重复率超过一定百分比则0分，否则10分</td><td>元数据</td></tr></tbody></table><h2 id="3-计算-Computation"><a href="#3-计算-Computation" class="headerlink" title="3. 计算 (Computation)"></a>3. 计算 (Computation)</h2><table><thead><tr><th>考评指标</th><th>评分标准</th><th>需要信息</th></tr></thead><tbody><tr><td>长期无产出 (Inactive Data)</td><td>近{days}天内无数据产出0分，否则10分</td><td>HDFS</td></tr><tr><td>长期无访问 (Unused Data)</td><td>近{days}天内无访问0分，否则10分</td><td>HDFS</td></tr><tr><td>计算任务有报错 (Job Errors)</td><td>任务执行有报错0分，无报错10分</td><td>任务信息</td></tr><tr><td>简单加工 (Simple Processing)</td><td>SQL无 <code>JOIN</code> &#x2F; <code>GROUP BY</code> &#x2F; 过滤操作则0分，否则10分</td><td>任务信息</td></tr></tbody></table><h2 id="4-质量-Data-Quality"><a href="#4-质量-Data-Quality" class="headerlink" title="4. 质量 (Data Quality)"></a>4. 质量 (Data Quality)</h2><table><thead><tr><th>考评指标</th><th>评分标准</th><th>需要信息</th></tr></thead><tbody><tr><td>表产出时效监控 (Data Timeliness)</td><td>产出时间超过历史均值一定比例则0分，否则10分</td><td>任务信息</td></tr><tr><td>表产出数据量监控 (Data Volume)</td><td>产出数据量超出历史均值上下限则0分，否则10分</td><td>HDFS</td></tr></tbody></table><h2 id="5-安全-Security"><a href="#5-安全-Security" class="headerlink" title="5. 安全 (Security)"></a>5. 安全 (Security)</h2><table><thead><tr><th>考评指标</th><th>评分标准</th><th>需要信息</th></tr></thead><tbody><tr><td>未明确安全等级 (Undefined Security Level)</td><td>未设置0分，已设置10分</td><td>元数据</td></tr><tr><td>目录文件访问权限 (Access Control)</td><td>目录或文件权限超出建议范围则0分，否则10分</td><td>HDFS</td></tr></tbody></table><hr><h2 id="评分规则"><a href="#评分规则" class="headerlink" title="评分规则"></a>评分规则</h2><p>数据治理考评平台会按照上述考评指标对企业的数据治理情况进行自动评估，并计算出最终得分，帮助企业发现问题并持续优化数据管理策略。</p><ol start="2"><li>考评引擎</li></ol><p>结合Hive元数据、HDFS存储信息、DolphinScheduler任务信息等计算评分<br>自动扫描数据仓库并生成考评报告<br>3. 数据治理可视化</p><p>通过Web页面展示各类考评指标及得分<br>详细展示每张表的元数据、存储情况、访问记录等<br>3. 技术栈：</p><p>后端：Spring Boot、MyBatis-Plus、MySQL<br>大数据组件：Hive、HDFS、DolphinScheduler<br>前端：基于Vue.js的可视化界面<br>4. 项目亮点：</p><p>量化考评机制：采用打分体系量化数据治理状况，清晰展示问题点<br>自动化数据获取：结合Hive、HDFS等多源数据，实时更新考评结果<br>高扩展性：支持定制考评规则，可适配不同企业需求</p><div class="video-container">[up主专用，视频内嵌代码贴在这]</div><style>.video-container {    position: relative;    width: 100%;    padding-top: 56.25%; /* 16:9 aspect ratio (height/width = 9/16 * 100%) */}.video-container iframe {    position: absolute;    top: 0;    left: 0;    width: 100%;    height: 100%;}</style><!-- ---title: 数据治理<p>date: 1739775481000<br>tags:<br>— –&gt;</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>数据中台</title>
      <link href="/wenzhang/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0/"/>
      <url>/wenzhang/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0/</url>
      
        <content type="html"><![CDATA[<p>json web token为了防止每次访问都登录，所以需要在客户端存储token，然后每次请求都带上token。</p><p>lambook少用</p><ol><li>应为它帮你插入什么你并不知道，还会导致代码混乱。(可能会插入equlas,hashcode等方法)</li><li>record语法出现</li></ol><p>get请求参数放在url中，post请求参数放在body中，get请求参数长度有限制，post请求参数长度无限制。<br>get会被浏览器缓存，post不会被缓存。</p><p>任务调度模块：</p><ol><li>定时任务：定时任务是指在指定的时间执行某些操作，比如每天凌晨1点执行某个任务。(cron表达式)</li><li>周期任务：周期任务是指按照固定时间间隔执行某些操作，比如每隔1小时执行某个任务。(quartz框架)</li><li>DAG任务：DAG任务是指有向无环图，它是一种任务调度模型，它将任务分解成多个子任务，每个子任务之间有依赖关系，只有前置任务执行完成后，才能执行后置任务。(airflow)</li></ol><p>为什么不用java timer？<br>因为java timer,如果java程序异常宕机，timer任务不会执行<br>quartz<br>可以将任务持久化到mysql数据库</p><p>antlr4</p><p>sql -&gt; 词法分析 -&gt; 标记流 -&gt; 语法分析 -&gt; 抽象语法树 -&gt; 语义分析 -&gt; 语义分析(类型检查) -&gt; 执行计划 -&gt; 将执行计划转换为物理计划 -&gt; 物理执行 -&gt; 结果集 -&gt; 输出</p><p>为什么不用mysql？<br>查询复杂</p><p>图数据库(专门用于处理图关系数据库)<br>neo4j，查询语言cypher不是sql</p><p>使用ai</p><ul><li>让ai复述需求</li><li>明确需求范围</li><li>需求拆解</li><li>把ai当小孩子，一步一步的教他</li></ul><div class="video-container">[up主专用，视频内嵌代码贴在这]</div><style>.video-container {    position: relative;    width: 100%;    padding-top: 56.25%; /* 16:9 aspect ratio (height/width = 9/16 * 100%) */}.video-container iframe {    position: absolute;    top: 0;    left: 0;    width: 100%;    height: 100%;}</style><!-- ---title: 数据中台<p>date: 1739254507000<br>tags:<br>— –&gt;</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>大数据优化</title>
      <link href="/wenzhang/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%BC%98%E5%8C%96/"/>
      <url>/wenzhang/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<p>主维表和相关维表关联时在不影响结果的情况下，尽量用left join，避免使用join。因为要保证相关维表丢数据影响主维表数据</p><p>两表join时，先where条件过滤，再join，避免join后数据量过大。</p><p>使用union all来合并数据(不影响结果的情况下)，而不是union，因为union会去重，增加负担。</p><p>用查询的方式修改数据，不用update语句</p><p>性能如何提升？</p><ul><li>join</li><li>可以减少数据量</li><li>存在重复计算</li><li>数据重复读取</li></ul><p>所有在nd表中用1d表减少重复计算</p><p>用炸裂函数减少io次数</p><ol><li>获取时间范围最大的数据集</li><li>将查询的数据集在内存中进行炸裂处理</li><li>将炸裂后的结果集筛选出有用的数据</li><li>将筛选后的数据照标记进行分组统计</li></ol><p>在7d和30d表中用1d表，30d表也算了7d表，可以加if判断，在30d表中算7d，但字段会变多</p><div class="video-container">[up主专用，视频内嵌代码贴在这]</div><style>.video-container {    position: relative;    width: 100%;    padding-top: 56.25%; /* 16:9 aspect ratio (height/width = 9/16 * 100%) */}.video-container iframe {    position: absolute;    top: 0;    left: 0;    width: 100%;    height: 100%;}</style><!-- ---title: 大数据优化<p>date: 1738671432000<br>tags:<br>— –&gt;</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>linux中出错处理方法</title>
      <link href="/wenzhang/Maxwell%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/"/>
      <url>/wenzhang/Maxwell%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="0-前言：这是错误的通用处理方法，我以maxwell启动失败为例，请根据具体情况进行调整。"><a href="#0-前言：这是错误的通用处理方法，我以maxwell启动失败为例，请根据具体情况进行调整。" class="headerlink" title="0. 前言：这是错误的通用处理方法，我以maxwell启动失败为例，请根据具体情况进行调整。"></a>0. 前言：这是错误的通用处理方法，我以maxwell启动失败为例，请根据具体情况进行调整。</h2><h2 id="1-查看日志文件"><a href="#1-查看日志文件" class="headerlink" title="1. 查看日志文件"></a>1. 查看日志文件</h2><p>找到maxwell的日志文件，一般在<code>~/maxwell/logs</code>目录下，根据具体情况进行调整。<br><img src="https://img01.zzh36111.us.kg/20250129160756.png" alt="这是我的日志文件目录"></p><h2 id="2-排查错误原因"><a href="#2-排查错误原因" class="headerlink" title="2. 排查错误原因"></a>2. 排查错误原因</h2><p>根据日志文件，找到error关键字，一般会有具体的错误原因。<br><img src="https://img01.zzh36111.us.kg/20250129161206.png" alt="这是我的日志文件中的error关键字"></p><h2 id="3-处理错误"><a href="#3-处理错误" class="headerlink" title="3. 处理错误"></a>3. 处理错误</h2><p>复制错误原因到搜索引擎或GPT，根据搜索结果进行错误处理。<br><img src="https://img01.zzh36111.us.kg/20250129162411.png" alt="根据GPT搜索结果排查下来发现是mysqlbinglog的配置问题"></p><div class="video-container">[up主专用，视频内嵌代码贴在这]</div><style>.video-container {    position: relative;    width: 100%;    padding-top: 56.25%; /* 16:9 aspect ratio (height/width = 9/16 * 100%) */}.video-container iframe {    position: absolute;    top: 0;    left: 0;    width: 100%;    height: 100%;}</style><!-- ---title: linux中出错处理方法<p>date: 1738137508000<br>tags:<br>— –&gt;</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>梳理</title>
      <link href="/wenzhang/%E6%A2%B3%E7%90%86/"/>
      <url>/wenzhang/%E6%A2%B3%E7%90%86/</url>
      
        <content type="html"><![CDATA[<ol><li>虚拟数据生成<br>&#x2F;opt&#x2F;module&#x2F;data_mocker目录下改<br>application.yml文件中的日期</li></ol><p>2.1 日志数据采集到kafka<br><code>这里加一个kafka的原因是：一是为了给实时数仓用，二是如果不加kafka,多个flume采集数据到hdfs如果文件重名，会导致数据写进hdfs失败(因为hdfs不支持随机写)，如果用flume聚合会导致第二个flume的source和sink传输数率不一致，导致chinle爆掉</code><br>2.2 业务数据采集到kafka<br>3. 从kafka把日志数据消费数据到hdfs<br>3.2 从kafka把业务数据消费数据到hdfs</p><ol start="4"><li>数仓建设(逻辑分层，物理没有分层)<br>4.1 ods建设</li></ol><p>4.1.1 为什么要建设ODS层？</p><p>因为我们的数仓需要数据，但是无法直接从业务系统获取<br>原因：1.业务数据库行式存储，数据仓库需要列式存储<br>      2.业务数据库不可以存储大量数据，数据仓库需要存储大量数据提升统计分析的准确性<br>      3.不应该让业务数据库对接数据仓库从而增加业务系统的负担</p><p>4.1.2 ODS层建设要点<br>因为我们想让统计分析的时间更长，所有需要采集的时间相对更短，所以我们让ods与业务系统同步数据，<strong>尽</strong>可能不变</p><ol><li>存储方式不要变</li><li>数据格式不要变</li><li>压缩格式不要变<br>可以变的</li><li>融合异构数据(列：要把日志数据存到一张表中，但是日志数据有不同的字段，所以要融合)</li><li>汇总不同时间数据(列：日志数据有不同的时间，需要汇总到一张表中)</li></ol><p>ODS层的设计格式如下：<br>（1）ODS层的表结构设计依托于从业务系统同步过来的数据结构。<br>（2）ODS层要保存全部历史数据，故其压缩格式应选择压缩比较高的，此处选择gzip。<br>（3）ODS层表名的命名规范为：ods_表名_单分区增量全量标识（inc&#x2F;full）。</p><p>4.1.3 日志数据表字段建设<br>日志数据为json格式，json表字段选最外层json对象属性为表字段，它是json的结构，所以用特殊类型</p><p>4.1.4 业务数据表字段建设<br>ods为数据仓库的数据源</p><p>4.2 dim建设(主要功能是分析数据：面相状态)</p><p>4.2.1 为什么要建设dim层？<br>因为不同的需求可以在同一个维度(角度)上进行分析，所有维度可能重复使用为了避免重复计算，所以我们建设dim层，为统计分析提供数据支撑。</p><p>DIM层设计要点：<br>（1）DIM层的设计依据是维度建模理论，该层存储维度模型的维度表。<br>（2）DIM层的数据存储格式为orc列式存储+snappy压缩。(因为经常要统计分析)<br>（3）DIM层表名的命名规范为dim_表名_全量表或者拉链表标识（full&#x2F;zip）</p><p>4.2.1 全量维度表建设<br>为什么用全量维度表？<br>数据仓库的一个重要特点就是反映历史的变化，所以如何保存维度的历史状态是维度设计的重要工作之一。所有要保存维度数据的历史状态，所以可以每天保存一份全量的维度数据。</p><p>为什么用拉链表？<br>拉链表的意义就在于能够更加高效的保存维度信息的历史状态(比全量表更加节省空间)<br>数据源为增量表(因为全量表得变化数据及其困难)</p><ol><li>确定维度（表）<br>在设计事实表时，已经确定了与每个事实表相关的维度，理论上每个相关维度均需对应一张维度表。需要注意到，可能存在多个事实表与同一个维度都相关的情况，这种情况需保证维度的唯一性，即只创建一张维度表。另外，如果某些维度表的维度属性很少，例如只有一个**名称，则可不创建该维度表，而把该表的维度属性直接增加到与之相关的事实表中，这个操作称为维度退化</li><li>确定主维表和相关维表(在业务数据库中查看)</li></ol><ul><li>主维表：主维表是指维度表的最底层，它存储维度的基本信息，比如维度名称、维度代码、维度描述等。</li><li>相关维表：相关维表是指维度表的中间层，它存储维度的关联信息，比如维度与主维表的关联关系、维度与其他维度的关联关系等。</li></ul><ol><li>确定维度属性(选择出可以用来分析的维度属性，主要看业务需求)</li></ol><ul><li>尽可能生成丰富的维度属性(列存储不会降低效率，但是会增加磁盘占用)</li><li>尽量不使用编码，而使用明确的文字说明，一般可以编码和文字共存(因为我们并不知道编码的含义)</li><li>尽量沉淀(计算)出通用的维度属性</li></ul><p>4.2.2 拉链维度表建设</p><p>4.3 dwd建设(主要功能是统计数据：面相行为)<br>4.3.1 为什么要建设dwd层？<br>对ods层的数据进行清洗、转换、加工，为后面统计分析做准备。<br>(这里加工为一个广泛概念，所有没有具体操作)</p><p>dwd主要保存业务行为数据(事实表)</p><p>大多数表是增量表，少部分是全量表</p><p>DWD层设计要点：<br>（1）DWD层的设计依据是维度建模理论，该层存储维度模型的事实表。<br>（2）DWD层的数据存储格式为orc列式存储+snappy压缩。(因为经常要统计分析)<br>（3）DWD层表名的命名规范为dwd_数据域_表名_单分区增量全量标识（inc&#x2F;full）</p><p>事实表应该包含以下信息：</p><ul><li>包含维度信息：维度越多描述行为越详细</li><li>包含度量值：所有行为必须可以用来统计，用来统计的值就是度量值</li></ul><p>设计事务事实表时一般可遵循以下四个步骤：</p><ul><li>选择业务过程：确定什么表</li><li>声明粒度：确定行</li><li>确认维度：确定列</li><li>确认事实：确定度量值</li></ul><p>4.4 dws建设<br>4.4.1 日志数据建设</p><p>4.4.2 业务数据建设<br>4.5 ads建设</p><div class="video-container">[up主专用，视频内嵌代码贴在这]</div><style>.video-container {    position: relative;    width: 100%;    padding-top: 56.25%; /* 16:9 aspect ratio (height/width = 9/16 * 100%) */}.video-container iframe {    position: absolute;    top: 0;    left: 0;    width: 100%;    height: 100%;}</style><!-- ---title: 梳理<p>date: 1738130258000<br>tags:<br>— –&gt;</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>建模</title>
      <link href="/wenzhang/%E5%BB%BA%E6%A8%A1/"/>
      <url>/wenzhang/%E5%BB%BA%E6%A8%A1/</url>
      
        <content type="html"><![CDATA[<ol><li>熟悉业务<br>MySQL保存的不是行为数据，而是实体数据，状态数据。</li></ol><p>数据源-》数据加工-》数据统计-》数据分析-》数据展示-》数据决策</p><p>为什么分多成，因为一个指标可能要重复运算</p><p>数据采集当天计算所以要求数据采集占用时间短，所以ods与采集到的数据格式尽可能不变</p><ol><li>存储方式</li><li>数据格式</li><li>压缩方式<br>有些可以变</li><li>融合异构</li><li>汇总不同时间数</li></ol><p>数仓目的是分析<br>分层设计为了使数据体系更加清晰，便于管理。</p><p>因为有很多数据重复所以有维度dim</p><ol><li>er：使用面向对象的方式设计表在</li><li>1对多的关系，多的表中建外键比在多的表中建外键查询更高效。</li><li>1对1：主外键相同</li><li>多对多采用两张表很难实现，第三张表中建立两个外键，分别指向主表和从表。</li></ol><p>3nf<br>函数依赖</p><p>1nf：每个属性都只包含一个值。不可分解。<br>2nf：在1nf基础上，非主属性不能直接依赖于主键。<br>3nf：在2nf基础上，任何非主属性不能传递依赖于主键。 </p><p>er模型表太多，不适合统计分析，所以需要维度建模进行分层设计。</p><ol start="5"><li>维度：</li><li>数据统计：汇总数据（行为的结果）——–事实表</li><li>数据分析：角度（状态）———维度表</li></ol><p>分层设计是逻辑分层，按表名分层。</p><p>ods主要存储所以用gzip压缩效率较高，且不用格式转换，且不用格式转换就可以建表<br>指定表位置方便管理<br>为什么用外部表？<br>看是否有别人一起用<br>分区表：按时间分区，方便查询，提高查询效率。（不用分区，会生成多文件，大文件，查询时会扫描所有文件，效率低）<br>分区字段不会存到文件会放到路径中，所以使用采用分区表<br>设置严格的权限控制，防止误操作<br>日志表字段怎么选？<br>如果json属性和表字段相同，可以正常解析<br>如果json属性少表字段存在的可以正常解析不存在的解析为null<br>如果json属性多表字段不存在的不解析<br>如果json属性和表字段不区分大小写，可以正常解析</p><p>所以json表字段选最外层json对象属性为表字段，它是json的结构，所以用特殊类型</p><p>map 与 struct 区别<br>map：value类型统一，key个数可以变<br>struct：value类型可以不统一，key个数固定</p><p>时间戳，10位为秒，13位为毫秒用bigint存储</p><p>因为 datebase,table 属性与业务无关所以建表时不要</p><p>ods是数仓数据源，dim和dwd才是统计分析数据源</p><p>dim表：<br>应该为列存储适合统计，hive：orc<br>所以压缩应该snappy<br>全量<br>状态数据为了避免出现问题最好每天存一份全部数据（绝大多数都维度表是全量表）<br>拉链<br>维度表<br>把有关联维度放到一张维度表，避免关联查询<br>如果维度表特别简单，可以不建维度表，直接在事实表中关联<br>只要可以用来分析的维度都是字段<br>确定字段来源（参考业务数据库的字段）<br>主维度：业务数据库主要分析字段的表<br>相关维度：业务数据库中存在关联关系的表<br>确定维度表字段：<br>字段越多越好（列存储不会影响查询效率）<br>编码和文字共存，<br>沉淀(计算)出通用属性</p><p>记得装载数据用 insert overwrite，不要用 insert into，因为insert into会导致数据重复。</p><p>数仓经常需要统计历史数据所有需要存储历史维度</p><p>全量快照表<br>1）离线数据仓库的计算周期通常为每天一次，所以可以每天保存一份全量的维度数据。这种方式的优点和缺点都很明显。<br>优点是简单而有效，开发和维护成本低，且方便理解和使用。<br>缺点是浪费存储空间，尤其是当数据的变化比例比较低时。大量数据没有变化<br>2）拉链表(要数据变化不频繁时)<br>拉链表的意义就在于能够更加高效的保存维度信息的历史状态</p><p>首日获取的全量数据只要用户的最新状态数据，不存在历史的状态数据，无法判断状态的开始<br>所有折中的认为首日就是当前状态的开始日期</p><p>分区策略<br>将数据存到结束时间所在分区</p><p>当天多次修改怎么办？</p><p>dwd层：</p><p>建表时进可以让粒度更细</p><p>事实表分类：<br>事务型事实表<br>绝大多数事实表</p><p>特殊需求，用以下方式建表为了提高效率：</p><p>周期性快照事实表：(特殊需求不需要关联计算直接如存量指标：存款，直接从业务数据库中周期性获取)因为特殊需求可能需要多个行为关联得到，所有有周期快照事实表</p><ul><li>日度快照事实表：每天更新一次(保存2状态数据，存量数据)</li><li>问为什么不作为维度表？因为这是度量值</li></ul><p>累计型快照事实表：(将一个流程的多个行为的状态累计在一个表中)</p><p>事实表应该包含以下信息：</p><ul><li>包含维度信息：维度越多描述行为越详细</li><li>包含度量值：所有行为必须可以用来统计，用来统计的值就是度量值</li></ul><p>设计事务事实表时一般可遵循以下四个步骤：<br>选择业务过程→声明粒度→确认维度→确认事实</p><p>dim表的数据来源：从业务数据库中确定主维表和相关维表，在从ods中找到该表</p><p>dwd表的数据来源：mysql不可以得到行为数据，只能得到状态数据，所以从maxwell中获取update,insert,delete操作的数据,就是行为数据(找到会应为行为变化的表)</p><p>首日获取的数据无法判断行为，所有，认为当前数据都是新增</p><p>建表-》load数据-》save数据</p><p>为什么dim大多是全量表dwd大多是增量表？<br>因为dim是状态数据，dwd是行为数据，状态数据需要全量，行为数据需要增量。</p><p>ADS<br>ads的数据是最终统计结果，所有不需要列存储，也不需要压缩snappy。<br>因为要同步到mysql，所有用行存储同步更快，文件格式tsv,压缩gzip。</p><p>因为要给客户看所有数据不会很多<br>所有ADS没有分区<br>ADS是客户要什么就建什么表</p><ul><li>统计周期：统计的时间范围</li><li>统计粒度：分析数据的角度(用那个维度分析)</li><li>指标：客户想要的结果数值<br>数据装载<br>-指标分析：</li><li>原子指标：业务过程(行为) + 度量值(要聚合的值) + 聚合逻辑(聚合函数)</li><li>派生指标：原子指标 + 统计周期(分区字段过滤，过滤的是文件夹) + 统计粒度(从哪个维度分析) + 业务限定(数据字段过滤，过滤的是文件)</li><li>衍生指标：通过多个派生指标计算得到的指标，如：比率、增长率、占比等</li></ul><p>dws层：<br>依赖于ads层<br>简化计算提前聚合，如果表可以在多个地方使用，可以提高效率<br>数据量大，要分区</p><p>1d数据来源：DIM,DWD<br>nd数据来源：必须为1d表<br>td数据来源：可以为1d表也可以为DIM表DWD表</p><p>表的设计：<br>参考ADS设计指标分析</p><ul><li>原子指标：行为 + 度量值 + 聚合逻辑</li><li>派生指标：原子指标 + 统计周期 + 统计粒度 + 业务限定</li><li>衍生指标：通过多个派生指标计算得到的指标，如：比率、增长率、占比等</li></ul><p>如果</p><ul><li>业务过程相同：数据来源相同</li><li>统计周期相同：数据分区相同</li><li>统计粒度相同：维度相同<br>这样的多个指标可以放在同一张表中，减少表的数量，提高查询效率。</li></ul><div class="video-container">[up主专用，视频内嵌代码贴在这]</div><style>.video-container {    position: relative;    width: 100%;    padding-top: 56.25%; /* 16:9 aspect ratio (height/width = 9/16 * 100%) */}.video-container iframe {    position: absolute;    top: 0;    left: 0;    width: 100%;    height: 100%;}</style><!-- ---title: 建模<p>date: 1737874753000<br>tags:<br>— –&gt;</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Datax</title>
      <link href="/wenzhang/Datax/"/>
      <url>/wenzhang/Datax/</url>
      
        <content type="html"><![CDATA[<h2 id="1-DataX概述"><a href="#1-DataX概述" class="headerlink" title="1.DataX概述"></a>1.DataX概述</h2><p>DataX是阿里巴巴开源的一个异构数据源离线同步工具，致力于实现包括关系型数据库（Mysql、Oracle等）、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。</p><h2 id="2-DataX作用"><a href="#2-DataX作用" class="headerlink" title="2.DataX作用"></a>2.DataX作用</h2><p><img src="https://img01.zzh36111.us.kg/20250124141748.png" alt="DataX作为中间传输载体负责连接数据各种数据源"></p><h2 id="3-DataX支持的数据源"><a href="#3-DataX支持的数据源" class="headerlink" title="3.DataX支持的数据源"></a>3.DataX支持的数据源</h2><p>DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NoSQL、大数据计算系统都已经接入，支持如下：</p><table><thead><tr><th>类型</th><th>数据源</th><th>Reader（读）</th><th>Writer（写）</th></tr></thead><tbody><tr><td>RDBMS 关系型数据库</td><td>MySQL</td><td>✔</td><td>✔</td></tr><tr><td></td><td>Oracle</td><td>✔</td><td>✔</td></tr><tr><td></td><td>OceanBase</td><td>✔</td><td>✔</td></tr><tr><td></td><td>SQLServer</td><td>✔</td><td>✔</td></tr><tr><td></td><td>PostgreSQL</td><td>✔</td><td>✔</td></tr><tr><td></td><td>DRDS</td><td>✔</td><td>✔</td></tr><tr><td></td><td>通用 RDBMS</td><td>✔</td><td>✔</td></tr><tr><td>阿里云数仓数据库</td><td>ODPS</td><td>✔</td><td>✔</td></tr><tr><td></td><td>ADS</td><td>✔</td><td>✔</td></tr><tr><td></td><td>OSS</td><td>✔</td><td>✔</td></tr><tr><td></td><td>OCS</td><td>✔</td><td>✔</td></tr><tr><td>NoSQL 数据存储</td><td>OTS</td><td>✔</td><td>✔</td></tr><tr><td></td><td>Hbase0.94</td><td>✔</td><td>✔</td></tr><tr><td></td><td>Hbase1.1</td><td>✔</td><td>✔</td></tr><tr><td></td><td>Phoenix4.x</td><td>✔</td><td>✔</td></tr><tr><td></td><td>Phoenix5.x</td><td>✔</td><td>✔</td></tr><tr><td></td><td>MongoDB</td><td>✔</td><td>✔</td></tr><tr><td></td><td>Hive</td><td>✔</td><td>✔</td></tr><tr><td></td><td>Cassandra</td><td>✔</td><td>✔</td></tr><tr><td>无结构化数据存储</td><td>TxtFile</td><td>✔</td><td>✔</td></tr><tr><td></td><td>FTP</td><td>✔</td><td>✔</td></tr><tr><td></td><td>HDFS</td><td>✔</td><td>✔</td></tr><tr><td></td><td>Elasticsearch</td><td>✔</td><td>✔</td></tr><tr><td>时间序列数据库</td><td>OpenTSDB</td><td>✔</td><td></td></tr><tr><td></td><td>TSDB</td><td>✔</td><td>✔</td></tr></tbody></table><h2 id="3-DataX使用"><a href="#3-DataX使用" class="headerlink" title="3.DataX使用"></a>3.DataX使用</h2><p>DataX的使用非常简单，用户仅需要根据自己同步数据的数据源和目的地的类型来选择相应的Reader和Writer插件即可，并将Reader和Writer插件的信息配置在一个json文件中，然后，在执行命令时，指定配置文件提交数据同步任务即可。</p><h3 id="3-1DataX配置文件格式"><a href="#3-1DataX配置文件格式" class="headerlink" title="3.1DataX配置文件格式"></a>3.1DataX配置文件格式</h3><p>这里以同步HDFS数据到MySQL为例，说明DataX的配置文件格式。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;job&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;setting&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;speed&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;channel&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;reader&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hdfsreader&quot;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/base_province&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;defaultFS&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hdfs://hadoop102:8020&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;column&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;*&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;fileType&quot;</span><span class="punctuation">:</span> <span class="string">&quot;text&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;compress&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gzip&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;encoding&quot;</span><span class="punctuation">:</span> <span class="string">&quot;UTF-8&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;nullFormat&quot;</span><span class="punctuation">:</span> <span class="string">&quot;\\N&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;fieldDelimiter&quot;</span><span class="punctuation">:</span> <span class="string">&quot;\t&quot;</span></span><br><span class="line">          <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;writer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mysqlwriter&quot;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;writeMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;replace&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;username&quot;</span><span class="punctuation">:</span> <span class="string">&quot;root&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;password&quot;</span><span class="punctuation">:</span> <span class="string">&quot;123456&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;column&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">              <span class="string">&quot;id&quot;</span><span class="punctuation">,</span></span><br><span class="line">              <span class="string">&quot;name&quot;</span><span class="punctuation">,</span></span><br><span class="line">              <span class="string">&quot;region_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">              <span class="string">&quot;area_code&quot;</span><span class="punctuation">,</span></span><br><span class="line">              <span class="string">&quot;iso_code&quot;</span><span class="punctuation">,</span></span><br><span class="line">              <span class="string">&quot;iso_3166_2&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;connection&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">              <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;jdbcUrl&quot;</span><span class="punctuation">:</span> <span class="string">&quot;jdbc:mysql://hadoop102:3306/gmall?useUnicode=true&amp;characterEncoding=utf-8&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;table&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                  <span class="string">&quot;test_province&quot;</span></span><br><span class="line">                <span class="punctuation">]</span></span><br><span class="line">              <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">          <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>Reader和Writer的具体参数可参考<a href="https://github.com/alibaba/DataX/blob/master/README.md">官方文档</a></p><h3 id="3-2DataX命令执行"><a href="#3-2DataX命令执行" class="headerlink" title="3.2DataX命令执行"></a>3.2DataX命令执行</h3><p>DataX的命令执行非常简单，只需要在命令行中执行如下命令：  </p><blockquote><p>python bin&#x2F;datax.py job&#x2F;base_province.json</p></blockquote><h5 id="3-2-1DataX传参"><a href="#3-2-1DataX传参" class="headerlink" title="3.2.1DataX传参"></a>3.2.1DataX传参</h5><p>在生产环境中，离线数据同步任务需要每日定时重复执行，故HDFS上的目标路径通常会包含一层日期，用来对每日同步的数据加以分区，也就是说每日同步数据的目标路径不是固定的，因此DataX配置文件中的DHFSWriter插件中参数path的值应该是动态变化的。为实现这个业务需求，我们需要使用DataX的传参功能<br><strong>DataX传参用法</strong><br>① 首选在任务的json配置文件中使用${param}引用参数<br>② 然后在提交任务时使用-p “-Dparam&#x3D;value” 传入参数值</p><h2 id="4-DataX调优"><a href="#4-DataX调优" class="headerlink" title="4.DataX调优"></a>4.DataX调优</h2><h3 id="4-1DataX运行流程"><a href="#4-1DataX运行流程" class="headerlink" title="4.1DataX运行流程"></a>4.1DataX运行流程</h3><p><img src="https://img01.zzh36111.us.kg/20250124214146.png"></p><h3 id="4-2DataX调度决策思路"><a href="#4-2DataX调度决策思路" class="headerlink" title="4.2DataX调度决策思路"></a>4.2DataX调度决策思路</h3><p>举例来说，用户提交了一个DataX作业，并且配置了总的并发度为20，目的是对一个有100张分表的mysql数据源进行同步。DataX的调度决策思路是：</p><ol><li>DataX Job根据分库分表切分策略(默认一个表一个Task，可以根据业务需要在reader插件中进行切分策略调整)，将同步工作分成100个Task。</li><li>DataX根据配置的总的并发度20，以及每个Task Group的并发度5，DataX计算共需要分配4个TaskGroup。</li><li>4个TaskGroup平分100个Task，每一个TaskGroup负责运行25个Task。</li></ol><h3 id="4-3DataX调优建议"><a href="#4-3DataX调优建议" class="headerlink" title="4.3DataX调优建议"></a>4.3DataX调优建议</h3><ol><li>速度控制:DataX中提供了包括通道（并发）、记录流、字节流三种流控模式，可以根据需要控制你的作业速度，让你的作业在数据库可以承受的范围内达到最佳的同步速度。</li><li>优化参数</li></ol><table><thead><tr><th>参数</th><th>说明</th><th>注意事项</th></tr></thead><tbody><tr><td><code>job.setting.speed.channel</code></td><td>并发数</td><td></td></tr><tr><td><code>job.setting.speed.record</code></td><td>总 record 限速</td><td>配置此参数，则必须配置单个 channel 的 record 限速参数</td></tr><tr><td><code>job.setting.speed.byte</code></td><td>总 byte 限速</td><td>配置此参数，则必须配置单个 channel 的 byte 限速参数</td></tr><tr><td><code>core.transport.channel.speed.record</code></td><td>单个 channel 的 record 限速，默认 10000 条&#x2F;s</td><td></td></tr><tr><td><code>core.transport.channel.speed.byte</code></td><td>单个 channel 的 byte 限速，默认 1M&#x2F;s</td><td></td></tr></tbody></table><p>注意：如果配置了总record限速和总byte限速，channel并发数就会失效。因为配置了这两个参数后，实际的channel并发数是通过计算得到的<br>①计算公式：<br>并发数&#x3D;min(byte_总⁄〖byte〗_单个channel ，  record_总⁄〖record〗_单个channel )<br>② 配置实例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;core&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;transport&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;channel&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;speed&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;byte&quot;</span><span class="punctuation">:</span> <span class="number">1048576</span> <span class="comment">//单个channel byte限速1M/s</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;job&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;setting&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;speed&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;byte&quot;</span> <span class="punctuation">:</span> <span class="number">5242880</span> <span class="comment">//总byte限速5M/s</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><!-- <div class="video-container">[up主专用，视频内嵌代码贴在这]</div> --><style>.video-container {    position: relative;    width: 100%;    padding-top: 56.25%; /* 16:9 aspect ratio (height/width = 9/16 * 100%) */}.video-container iframe {    position: absolute;    top: 0;    left: 0;    width: 100%;    height: 100%;}</style><!-- ---title: Datax<p>date: 1737698997000<br>tags:<br>— –&gt;</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>如何使用Maxwell进行全量表或增量表的数据同步</title>
      <link href="/wenzhang/Maxwell/"/>
      <url>/wenzhang/Maxwell/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Maxwell概述"><a href="#1-Maxwell概述" class="headerlink" title="1.Maxwell概述:"></a>1.Maxwell概述:</h2><p>Maxwell是使用Java编写(可以使用jps查看进程)的MySQL变更数据抓取软件。他会实时监控Mysql数据库的数据变更操作（包括insert、update、delete），并将变更数据以JSON的格式发送给Kafka、Kinesi等流数据处理平台。</p><h2 id="Maxwell输出数据格式"><a href="#Maxwell输出数据格式" class="headerlink" title="Maxwell输出数据格式"></a>Maxwell输出数据格式</h2><table><thead><tr><th>操作</th><th>SQL语句</th><th>数据格式</th></tr></thead><tbody><tr><td>插入</td><td><code>insert into gmall.student values(1, &#39;zhangsan&#39;);</code></td><td>&#96;&#96;&#96;json</td></tr><tr><td></td><td></td><td>{</td></tr><tr><td></td><td></td><td>“database”: “gmall”,</td></tr><tr><td></td><td></td><td>“table”: “student”,</td></tr><tr><td></td><td></td><td>“type”: “insert”,</td></tr><tr><td></td><td></td><td>“ts”: 1634004537,</td></tr><tr><td></td><td></td><td>“xid”: 1530907,</td></tr><tr><td></td><td></td><td>“commit”: true,</td></tr><tr><td></td><td></td><td>“data”: {</td></tr><tr><td></td><td></td><td>“id”: 1,</td></tr><tr><td></td><td></td><td>“name”: “zhangsan”</td></tr><tr><td></td><td></td><td>}</td></tr><tr><td></td><td></td><td>}</td></tr><tr><td></td><td></td><td>&#96;&#96;&#96;</td></tr><tr><td>更新</td><td><code>update gmall.student set name = &#39;lisi&#39; where id=1;</code></td><td>&#96;&#96;&#96;json</td></tr><tr><td></td><td></td><td>{</td></tr><tr><td></td><td></td><td>“database”: “gmall”,</td></tr><tr><td></td><td></td><td>“table”: “student”,</td></tr><tr><td></td><td></td><td>“type”: “update”,</td></tr><tr><td></td><td></td><td>“ts”: 1634004657,</td></tr><tr><td></td><td></td><td>“xid”: 1531916,</td></tr><tr><td></td><td></td><td>“commit”: true,</td></tr><tr><td></td><td></td><td>“data”: {</td></tr><tr><td></td><td></td><td>“id”: 1,</td></tr><tr><td></td><td></td><td>“name”: “lisi”</td></tr><tr><td></td><td></td><td>},</td></tr><tr><td></td><td></td><td>“old”: {</td></tr><tr><td></td><td></td><td>“name”: “zhangsan”</td></tr><tr><td></td><td></td><td>}</td></tr><tr><td></td><td></td><td>}</td></tr><tr><td></td><td></td><td>&#96;&#96;&#96;</td></tr><tr><td>删除</td><td><code>delete from gmall.student where id=1;</code></td><td>&#96;&#96;&#96;json</td></tr><tr><td></td><td></td><td>{</td></tr><tr><td></td><td></td><td>“database”: “gmall”,</td></tr><tr><td></td><td></td><td>“table”: “student”,</td></tr><tr><td></td><td></td><td>“type”: “delete”,</td></tr><tr><td></td><td></td><td>“ts”: 1634004751,</td></tr><tr><td></td><td></td><td>“xid”: 1532725,</td></tr><tr><td></td><td></td><td>“commit”: true,</td></tr><tr><td></td><td></td><td>“data”: {</td></tr><tr><td></td><td></td><td>“id”: 1,</td></tr><tr><td></td><td></td><td>“name”: “lisi”</td></tr><tr><td></td><td></td><td>}</td></tr><tr><td></td><td></td><td>}</td></tr><tr><td></td><td></td><td>&#96;&#96;&#96;</td></tr></tbody></table><p><strong>字段说明：</strong></p><ol><li><code>database</code>：变更数据所属的数据库</li><li><code>commit</code>：事务提交标志，可用于重新组装事务。 </li><li>xid：事务ID，用于标识事务。 </li><li>ts：变更数据的时间戳。 </li><li>type：变更数据的类型，包括insert、update、delete。 </li><li>data：变更数据，包括新增或修改的数据。 </li><li>old：变更前的数据，仅在update操作时存在。</li></ol><h2 id="2-Maxwell的实现原理"><a href="#2-Maxwell的实现原理" class="headerlink" title="2.Maxwell的实现原理"></a>2.Maxwell的实现原理</h2><p>Maxwell的实现原理很简单，就是将自己伪装成Slave，并遵循Mysql主从复制的协议，从master中同步数据。<br><strong>Mysql主从复制工作原理</strong><img src="https://img01.zzh36111.us.kg/20250123182703.png"><br>① Master主库接收到数据变更请求，完成数据变更，并将其写到二级制日志（binary log）中。<br>② Slave从库向Mysql master发送dump协议，将Master主库的binary log events拷贝到从库的中继日志（relay log）中。<br>③ Slave从库读取并回放中继日志中的事件，将更新的数据同步到自己的数据库中。</p><h2 id="3-Maxwell部署"><a href="#3-Maxwell部署" class="headerlink" title="3.Maxwell部署"></a>3.Maxwell部署</h2><h3 id="1-配置MySQL"><a href="#1-配置MySQL" class="headerlink" title="1.配置MySQL"></a>1.配置MySQL</h3><h4 id="1-1"><a href="#1-1" class="headerlink" title="1.1"></a>1.1</h4><p>配置MySQL启动MySQL Binlog（修改MySQL的配置文件&#x2F;etc&#x2F;my.cnf重启Mysql服务后才生效）</p><h4 id="2-1"><a href="#2-1" class="headerlink" title="2.1"></a>2.1</h4><p>Mysql Binlog模式介绍<br>① Statement-based: 基于语句，Binlog会记录所有写操作的SQL语句，包括：insert、update、delete等。<br>                    优点：节省空间；<br>                    缺点：有可能造成数据不一致，如：insert语句中包含now()函数。<br>② Row-based：       基于行，Binlog会记录每次写操作的被操作行记录的变化。<br>                    优点：保持数据的绝对一致性<br>                    缺点：占用较大空间<br>③ mixed：   混合模式，默认是Statement-based，如果SQL语句可能导致数据不一致，就自动切换到Row-based<br>Maxwell要求Binlog采用Row-based模式</p><h3 id="2-创建Maxwell所需数据库和用户"><a href="#2-创建Maxwell所需数据库和用户" class="headerlink" title="2.创建Maxwell所需数据库和用户"></a>2.创建Maxwell所需数据库和用户</h3><h3 id="3-配置Maxwell"><a href="#3-配置Maxwell" class="headerlink" title="3.配置Maxwell"></a>3.配置Maxwell</h3><h4 id="3-1"><a href="#3-1" class="headerlink" title="3.1"></a>3.1</h4><p>修改Maxwell的配置文件config.properties,主要是配置数据发送的目的地。（若Maxwell发送数据的目的地是kafka集群，需要首先将kafka集群启动。）</p><h2 id="4-启动Maxwell"><a href="#4-启动Maxwell" class="headerlink" title="4.启动Maxwell"></a>4.启动Maxwell</h2><p>Maxwell的启动命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/maxwell --config config.properties --daemon</span><br></pre></td></tr></table></figure><p>Maxwell关闭命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep maxwell | grep -v grep | grep maxwell | awk &#x27;&#123;print $2&#125;&#x27; | xargs kill -9</span><br></pre></td></tr></table></figure><h3 id="Maxwell全量数据同步流程命令"><a href="#Maxwell全量数据同步流程命令" class="headerlink" title="Maxwell全量数据同步流程命令"></a>Maxwell全量数据同步流程命令</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/maxwell-bootstrap --database edu --table user_info --config config.properties</span><br></pre></td></tr></table></figure><!-- <div class="video-container">[up主专用，视频内嵌代码贴在这]</div> --><style>.video-container {    position: relative;    width: 100%;    padding-top: 56.25%; /* 16:9 aspect ratio (height/width = 9/16 * 100%) */}.video-container iframe {    position: absolute;    top: 0;    left: 0;    width: 100%;    height: 100%;}</style><!-- ---title: 如何使用Maxwell进行全量表或增量表的数据同步<p>date: 1737624278000<br>tags:<br>— –&gt;</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>在线教育离线数仓之数据采集</title>
      <link href="/wenzhang/%E5%9C%A8%E7%BA%BF%E6%95%99%E8%82%B2%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"/>
      <url>/wenzhang/%E5%9C%A8%E7%BA%BF%E6%95%99%E8%82%B2%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<!-- <div class="video-container">[up主专用，视频内嵌代码贴在这]</div> --><style>.video-container {    position: relative;    width: 100%;    padding-top: 56.25%; /* 16:9 aspect ratio (height/width = 9/16 * 100%) */}.video-container iframe {    position: absolute;    top: 0;    left: 0;    width: 100%;    height: 100%;}</style><!-- ---title: 在线教育离线数仓之数据采集<p>date: 1737542912000<br>tags:<br>— –&gt;</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/wenzhang/hello-world/"/>
      <url>/wenzhang/hello-world/</url>
      
        <content type="html"><![CDATA[<h1 id="博客预览"><a href="#博客预览" class="headerlink" title="博客预览"></a>博客预览</h1><blockquote><p>hexo cl; hexo s</p></blockquote><h1 id="推送"><a href="#推送" class="headerlink" title="推送"></a>推送</h1><blockquote><p>hexo cl; hexo g; hexo d</p></blockquote><h3 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h3><h2 id="基本语法-1"><a href="#基本语法-1" class="headerlink" title="基本语法"></a>基本语法</h2><h3 id="基本语法-2"><a href="#基本语法-2" class="headerlink" title="基本语法"></a>基本语法</h3><p><strong>引用</strong></p><ol><li>hjjk </li><li>jkdsfjl</li></ol><ul><li>skolfakashjd </li><li>kdsfjfjljds<br>djlksgjdfjksldfjl</li></ul><ul><li>sakllflj</li></ul><ul><li>sakhaklfhk</li></ul><ol><li>lsalk</li></ol><table><thead><tr><th>表头 1</th><th>表头 2</th></tr></thead><tbody><tr><td>内容 1</td><td>内容 2</td></tr><tr><td>内容 3</td><td>内容 4</td></tr></tbody></table><div style="text-align: center;"><table><thead><tr><th align="right">kalfl</th><th align="center">ldskkfldg</th></tr></thead><tbody><tr><td align="right">lslfjkk</td><td align="center">kdjfglk</td></tr></tbody></table></div><div style="text-align: center;"><table><thead><tr><th>服务名称</th><th>子服务</th><th>hadoop102</th><th>hadoop103</th><th>hadoop104</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode</td><td>√</td><td></td><td></td></tr><tr><td>HDFS</td><td>DataNode</td><td>√</td><td>√</td><td>√</td></tr><tr><td>HDFS</td><td>SecondaryNameNode</td><td></td><td></td><td>√</td></tr><tr><td>Yarn</td><td>NodeManager</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Yarn</td><td>Resourcemanager</td><td></td><td>√</td><td></td></tr><tr><td>Zookeeper</td><td>Zookeeper Server</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Flume(采集日志)</td><td>Flume</td><td>√</td><td>√</td><td></td></tr><tr><td>Kafka</td><td>Kafka</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Flume（消费Kafka日志）</td><td>Flume</td><td></td><td></td><td>√</td></tr><tr><td>Flume（消费Kafka业务）</td><td>Flume</td><td></td><td></td><td>√</td></tr><tr><td>Hive</td><td>Hive</td><td>√</td><td></td><td></td></tr><tr><td>MySQL</td><td>MySQL</td><td>√</td><td></td><td></td></tr><tr><td>DataX</td><td>DataX</td><td>√</td><td></td><td></td></tr><tr><td>Maxwell</td><td>Maxwell</td><td>√</td><td></td><td></td></tr><tr><td>Spark</td><td></td><td>√</td><td>√</td><td>√</td></tr><tr><td>ClickHouse</td><td></td><td>√</td><td></td><td></td></tr><tr><td>DolphinScheduler</td><td>ApiApplicationServer</td><td>√</td><td></td><td></td></tr><tr><td>DolphinScheduler</td><td>AlertServer</td><td>√</td><td></td><td></td></tr><tr><td>DolphinScheduler</td><td>MasterServer</td><td>√</td><td></td><td></td></tr><tr><td>DolphinScheduler</td><td>WorkerServer</td><td>√</td><td>√</td><td>√</td></tr><tr><td>DolphinScheduler</td><td>LoggerServer</td><td>√</td><td>√</td><td>√</td></tr></tbody></table><p>格式化 alt+shift+f</p></div>`dfhjkgh`<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lhjsdfkhs</span><br><span class="line">jkhdsfkj</span><br></pre></td></tr></table></figure><p>alt+shift+f格式化表格</p><p><a href="https://cmliussss.com/p/HexoBlogNo2/">超链接</a>：<a href="https://cmliussss.com/p/HexoBlogNo2/">https://cmliussss.com/p/HexoBlogNo2/</a></p><p>请<a href="https://cmliussss.com/p/HexoBlogNo2/">点我</a></p><h1 id="服务部署情况"><a href="#服务部署情况" class="headerlink" title="服务部署情况"></a>服务部署情况</h1><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>在线教育离线数仓使用的脚本合集</title>
      <link href="/wenzhang/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E9%A1%B9%E7%9B%AE/"/>
      <url>/wenzhang/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E9%A1%B9%E7%9B%AE/</url>
      
        <content type="html"><![CDATA[<h3 id="1-集群执行命令脚本："><a href="#1-集群执行命令脚本：" class="headerlink" title="1. 集群执行命令脚本："></a>1. 集群执行命令脚本：</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">! /bin/bash</span></span><br><span class="line"> </span><br><span class="line">for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">    echo --------- $i ----------</span><br><span class="line">    ssh $i &quot;$*&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h3 id="2-集群同步脚本"><a href="#2-集群同步脚本" class="headerlink" title="2. 集群同步脚本"></a>2. 集群同步脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">1. 判断参数个数</span></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">  echo Not Enough Arguement!</span><br><span class="line">  exit;</span><br><span class="line">fi</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">2. 遍历集群所有机器</span></span><br><span class="line">for host in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">  echo ====================  $host  ====================</span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">3. 遍历所有目录，挨个发送</span></span><br><span class="line">  for file in $@</span><br><span class="line">  do</span><br><span class="line">    #4 判断文件是否存在</span><br><span class="line">    if [ -e $file ]</span><br><span class="line">    then</span><br><span class="line">      #5. 获取父目录(dirname获取绝对路径父目录,相对路径获取的是.）</span><br><span class="line">      pdir=$(cd -P $(dirname $file); pwd)</span><br><span class="line">      #6. 获取当前文件的名称</span><br><span class="line">      fname=$(basename $file)</span><br><span class="line">      ssh $host &quot;mkdir -p $pdir&quot;</span><br><span class="line">      rsync -av $pdir/$fname $host:$pdir #-av是打印进度的</span><br><span class="line">    else</span><br><span class="line">      echo $file does not exists!</span><br><span class="line">    fi</span><br><span class="line">  done</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h3 id="3-Hadoop启动脚本"><a href="#3-Hadoop启动脚本" class="headerlink" title="3. Hadoop启动脚本"></a>3. Hadoop启动脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; =================== 启动 hadoop集群 ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot; --------------- 启动 hdfs ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop/sbin/start-dfs.sh&quot;</span><br><span class="line">        echo &quot; --------------- 启动 yarn ---------------&quot;</span><br><span class="line">        ssh hadoop103 &quot;/opt/module/hadoop/sbin/start-yarn.sh&quot;</span><br><span class="line">        echo &quot; --------------- 启动 historyserver ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop/bin/mapred --daemon start historyserver&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; =================== 关闭 hadoop集群 ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot; --------------- 关闭 historyserver ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop/bin/mapred --daemon stop historyserver&quot;</span><br><span class="line">        echo &quot; --------------- 关闭 yarn ---------------&quot;</span><br><span class="line">        ssh hadoop103 &quot;/opt/module/hadoop/sbin/stop-yarn.sh&quot;</span><br><span class="line">        echo &quot; --------------- 关闭 hdfs ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop/sbin/stop-dfs.sh&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h3 id="4-Zookeeper启动脚本"><a href="#4-Zookeeper启动脚本" class="headerlink" title="4. Zookeeper启动脚本"></a>4. Zookeeper启动脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">判断是否输入参数</span></span><br><span class="line">if [ $# -lt 1 ]; then</span><br><span class="line">  echo &quot;请输入参数&quot;</span><br><span class="line">  exit</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">        for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">        do</span><br><span class="line">        echo ---------- zookeeper $i 启动 ------------</span><br><span class="line">                ssh $i &quot;/opt/module/zookeeper/bin/zkServer.sh start&quot;</span><br><span class="line">        done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">        for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">        do</span><br><span class="line">        echo ---------- zookeeper $i 停止 ------------    </span><br><span class="line">                ssh $i &quot;/opt/module/zookeeper/bin/zkServer.sh stop&quot;</span><br><span class="line">        done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;status&quot;)&#123;</span><br><span class="line">        for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">        do</span><br><span class="line">        echo ---------- zookeeper $i 状态 ------------    </span><br><span class="line">                ssh $i &quot;/opt/module/zookeeper/bin/zkServer.sh status&quot;</span><br><span class="line">        done</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h3 id="5-Kafka启动脚本"><a href="#5-Kafka启动脚本" class="headerlink" title="5. Kafka启动脚本"></a>5. Kafka启动脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">! /bin/bash</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">判断是否输入参数</span></span><br><span class="line">if [ $# -lt 1 ]; then</span><br><span class="line">  echo &quot;请输入参数&quot;</span><br><span class="line">  exit</span><br><span class="line">fi</span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">    for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">    do</span><br><span class="line">        echo &quot; --------启动 $i Kafka-------&quot;</span><br><span class="line">        ssh $i &quot;/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties&quot;</span><br><span class="line">    done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">    for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">    do</span><br><span class="line">        echo &quot; --------停止 $i Kafka-------&quot;</span><br><span class="line">        ssh $i &quot;/opt/module/kafka/bin/kafka-server-stop.sh &quot;</span><br><span class="line">    done</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p>maxwell启动脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">MAXWELL_HOME=/opt/module/maxwell</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">grep -v grep是为了过滤掉grep本身的进程</span></span><br><span class="line">result=$(ps -ef | grep Maxwell | grep -v grep)</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">start )</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">判断result是否为空</span></span><br><span class="line">    if [ -z &quot;$result&quot; ]; then</span><br><span class="line">        echo &quot;启动Maxwell&quot;</span><br><span class="line">        $MAXWELL_HOME/bin/maxwell --config $MAXWELL_HOME/config.properties --daemon</span><br><span class="line">    else</span><br><span class="line">         echo &quot;Maxwell正在运行&quot;</span><br><span class="line">    fi</span><br><span class="line">;;</span><br><span class="line">stop )</span><br><span class="line">    if [ -z &quot;$result&quot; ]; then</span><br><span class="line">        echo &quot;Maxwell未在运行&quot;</span><br><span class="line">    else</span><br><span class="line">echo &quot;停止Maxwell&quot;</span><br><span class="line">        ps -ef | grep Maxwell | grep -v grep | awk &#x27;&#123;print $2&#125;&#x27; | xargs kill -9</span><br><span class="line">    fi</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><!-- <div class="video-container">[up主专用，视频内嵌代码贴在这]</div> --><style>.video-container {    position: relative;    width: 100%;    padding-top: 56.25%; /* 16:9 aspect ratio (height/width = 9/16 * 100%) */}.video-container iframe {    position: absolute;    top: 0;    left: 0;    width: 100%;    height: 100%;}</style>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>大数据项目之在线教育离线数仓</title>
      <link href="/wenzhang/%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E6%96%B0%E7%9A%84%E5%8D%9A%E6%96%87/"/>
      <url>/wenzhang/%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E6%96%B0%E7%9A%84%E5%8D%9A%E6%96%87/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据开发项目之在线教育离线数仓从零到一（学习项目）"><a href="#大数据开发项目之在线教育离线数仓从零到一（学习项目）" class="headerlink" title="大数据开发项目之在线教育离线数仓从零到一（学习项目）"></a>大数据开发项目之在线教育离线数仓从零到一（学习项目）</h1><p>本文详细介绍了大数据开发项目之在线教育离线数仓的开发过程，从零到一，包括项目背景、需求分析、设计思路、技术选型、开发环境搭建、数据采集、数仓搭建、</p><h2 id="项目整体流程"><a href="#项目整体流程" class="headerlink" title="项目整体流程"></a>项目整体流程</h2><p>该项目是一个在线教育大数据开发项目，旨在构建离线数仓，对在线教育业务数据进行分析处理，以支持企业决策。以下是项目的详细流程：</p><h3 id="1-业务流程与数据来源"><a href="#1-业务流程与数据来源" class="headerlink" title="1. 业务流程与数据来源"></a>1. 业务流程与数据来源</h3><ul><li><strong>业务流程</strong>：用户从在线教育网站首页开始浏览课程，可通过分类查询或全文检索寻找课程，找到后可添加到购物车、登录、结算，生成订单和支付数据，订单生成后会进行跟踪处理。</li><li><strong>数据来源</strong>：包括用户行为数据（通过前端埋点采集，存储在 HDFS 文件中）和业务数据（存储在 MySQL 中）。(因为本项目为学习项目，所以数据来源均为数据模拟器模拟的虚拟数据)</li></ul><h3 id="2-系统数据流程设计"><a href="#2-系统数据流程设计" class="headerlink" title="2. 系统数据流程设计"></a>2. 系统数据流程设计</h3><ul><li><strong>集群流程图</strong>：业务服务器与 App 业务交互，通过 Nginx 进行数据传输。日志数据经日志服务器采集，可采用 flume 采集方式，部分数据经消息缓存后存入 Kafka。业务数据通过 DataX 每日同步从 MySQL 数据库导入。数据在集群中经过 ODS、DWD、DWS 等层的处理，最终用于可视化。<br><img src="https://img01.zzh36111.us.kg/20250122151413.png" alt="流程图"><br>问：为什么不直接使用flume把数据采集到hdfs，要在之间加一个kafka呢？可以把Kafka换为flume聚合吗？</li><li><strong>集群特点</strong>：具备多数据源对接能力，可进行离线批量和在线实时处理，有统一的集群管理配置监控平台，并实现用户认证和权限管理，满足多租户需求。</li></ul><h3 id="3-技术选型"><a href="#3-技术选型" class="headerlink" title="3. 技术选型"></a>3. 技术选型</h3><ul><li><p><strong>Apache 框架版本</strong>：确定了 Hadoop、Flume、Kafka、Hive、Sqoop 等多种技术框架的具体版本，每个版本都有其特定的功能特点，以满足项目需求。</p><table><thead><tr><th>框架</th><th>版本</th></tr></thead><tbody><tr><td>Hadoop</td><td>3.1.3</td></tr><tr><td>Zookeeper</td><td>3.5.7</td></tr><tr><td>MySQL</td><td>5.7.16</td></tr><tr><td>Hive</td><td>3.1.2</td></tr><tr><td>Flume</td><td>1.9.0</td></tr><tr><td>Kafka</td><td>3.0.0</td></tr><tr><td>Spark</td><td>3.0.0</td></tr><tr><td>DataX</td><td>3.0.0</td></tr><tr><td>Superset</td><td>1.3.2</td></tr><tr><td>DolphinScheduler</td><td>2.0.3</td></tr><tr><td>Maxwell</td><td>1.29.2</td></tr></tbody></table></li><li><p><strong>服务器选型</strong>：因为本项目仅为学习项目所以服务器选型比较简单仅为三台虚拟机。</p></li></ul><h3 id="4-集群规模规划"><a href="#4-集群规模规划" class="headerlink" title="4. 集群规模规划"></a>4. 集群规模规划</h3><ul><li><p><strong>集群规划</strong>：详细规划了各服务器节点上部署的组件，如 DataNode、NodeManager、ResourceManager 等在不同服务器上的分布。</p><table><thead><tr><th>服务名称</th><th>子服务</th><th>hadoop102</th><th>hadoop103</th><th>hadoop104</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode</td><td>√</td><td></td><td></td></tr><tr><td>HDFS</td><td>DataNode</td><td>√</td><td>√</td><td>√</td></tr><tr><td>HDFS</td><td>SecondaryNameNode</td><td></td><td></td><td>√</td></tr><tr><td>Yarn</td><td>NodeManager</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Yarn</td><td>Resourcemanager</td><td></td><td>√</td><td></td></tr><tr><td>Zookeeper</td><td>Zookeeper Server</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Flume(采集日志)</td><td>Flume</td><td>√</td><td>√</td><td></td></tr><tr><td>Kafka</td><td>Kafka</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Flume（消费Kafka日志）</td><td>Flume</td><td></td><td></td><td>√</td></tr><tr><td>Flume（消费Kafka业务）</td><td>Flume</td><td></td><td></td><td>√</td></tr><tr><td>Hive</td><td>Hive</td><td>√</td><td></td><td></td></tr><tr><td>MySQL</td><td>MySQL</td><td>√</td><td></td><td></td></tr><tr><td>DataX</td><td>DataX</td><td>√</td><td></td><td></td></tr><tr><td>Maxwell</td><td>Maxwell</td><td>√</td><td></td><td></td></tr><tr><td>Spark</td><td></td><td>√</td><td>√</td><td>√</td></tr><tr><td>DolphinScheduler</td><td>ApiApplicationServer</td><td>√</td><td></td><td></td></tr><tr><td>DolphinScheduler</td><td>AlertServer</td><td>√</td><td></td><td></td></tr><tr><td>DolphinScheduler</td><td>MasterServer</td><td>√</td><td></td><td></td></tr><tr><td>DolphinScheduler</td><td>WorkerServer</td><td>√</td><td>√</td><td>√</td></tr><tr><td>DolphinScheduler</td><td>LoggerServer</td><td>√</td><td>√</td><td>√</td></tr></tbody></table></li></ul><h3 id="5-数据仓库学习重点"><a href="#5-数据仓库学习重点" class="headerlink" title="5. 数据仓库学习重点"></a>5. 数据仓库学习重点</h3><ol><li>建模（建表）</li><li>SQL及优化</li><li>任务调度器</li></ol><p>如果直接将MySQL作为数据仓库数据源</p><ol><li>MySQL为行存储，数仓为列存储</li><li>MySQL不是海量数据，数据仓库要求海量数据（因为数据量越大，分析结果越准确）</li><li>会降低MySQL的查询性能</li><li>所以数仓设计自己的数据源要求汇总数据</li><li>加hdfs(因为数仓采用hive)为解耦合，数据源和数仓分离，数据源只负责采集数据，数仓只负责分析数据</li><li></li></ol><!-- ### 5. 数据生成与分析指标 - **数据生成器使用**：通过上传相关配置文件（application.yml、path.json、logback.xml 等）到指定目录，并修改配置文件参数，可在虚拟机指定路径下生成日志数据并向业务数据库插入业务数据。可生成不同日期数据，且能灵活配置用户点击路径和日志生成路径。 - **分析指标**：涵盖流量主题（如各来源流量统计、路径分析、各来源下单统计）、用户主题（如用户变动统计、留存率、新增活跃统计等）、课程主题（如各分类课程交易统计、评价统计、试听留存统计等）、交易主题、考试主题、播放主题、完课主题等多方面的指标，每个指标都有明确的统计周期和粒度要求。 -->]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
