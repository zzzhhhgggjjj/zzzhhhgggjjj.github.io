<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Datax</title>
      <link href="/wenzhang/Datax/"/>
      <url>/wenzhang/Datax/</url>
      
        <content type="html"><![CDATA[<h2 id="1-DataX概述"><a href="#1-DataX概述" class="headerlink" title="1.DataX概述"></a>1.DataX概述</h2><p>DataX是阿里巴巴开源的一个异构数据源离线同步工具，致力于实现包括关系型数据库（Mysql、Oracle等）、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。</p><h2 id="2-DataX作用"><a href="#2-DataX作用" class="headerlink" title="2.DataX作用"></a>2.DataX作用</h2><p><img src="https://img01.zzh36111.us.kg/20250124141748.png" alt="DataX作为中间传输载体负责连接数据各种数据源"></p><h2 id="3-DataX支持的数据源"><a href="#3-DataX支持的数据源" class="headerlink" title="3.DataX支持的数据源"></a>3.DataX支持的数据源</h2><p>DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NoSQL、大数据计算系统都已经接入，支持如下：</p><table><thead><tr><th>类型</th><th>数据源</th><th>Reader（读）</th><th>Writer（写）</th></tr></thead><tbody><tr><td>RDBMS 关系型数据库</td><td>MySQL</td><td>✔</td><td>✔</td></tr><tr><td></td><td>Oracle</td><td>✔</td><td>✔</td></tr><tr><td></td><td>OceanBase</td><td>✔</td><td>✔</td></tr><tr><td></td><td>SQLServer</td><td>✔</td><td>✔</td></tr><tr><td></td><td>PostgreSQL</td><td>✔</td><td>✔</td></tr><tr><td></td><td>DRDS</td><td>✔</td><td>✔</td></tr><tr><td></td><td>通用 RDBMS</td><td>✔</td><td>✔</td></tr><tr><td>阿里云数仓数据库</td><td>ODPS</td><td>✔</td><td>✔</td></tr><tr><td></td><td>ADS</td><td>✔</td><td>✔</td></tr><tr><td></td><td>OSS</td><td>✔</td><td>✔</td></tr><tr><td></td><td>OCS</td><td>✔</td><td>✔</td></tr><tr><td>NoSQL 数据存储</td><td>OTS</td><td>✔</td><td>✔</td></tr><tr><td></td><td>Hbase0.94</td><td>✔</td><td>✔</td></tr><tr><td></td><td>Hbase1.1</td><td>✔</td><td>✔</td></tr><tr><td></td><td>Phoenix4.x</td><td>✔</td><td>✔</td></tr><tr><td></td><td>Phoenix5.x</td><td>✔</td><td>✔</td></tr><tr><td></td><td>MongoDB</td><td>✔</td><td>✔</td></tr><tr><td></td><td>Hive</td><td>✔</td><td>✔</td></tr><tr><td></td><td>Cassandra</td><td>✔</td><td>✔</td></tr><tr><td>无结构化数据存储</td><td>TxtFile</td><td>✔</td><td>✔</td></tr><tr><td></td><td>FTP</td><td>✔</td><td>✔</td></tr><tr><td></td><td>HDFS</td><td>✔</td><td>✔</td></tr><tr><td></td><td>Elasticsearch</td><td>✔</td><td>✔</td></tr><tr><td>时间序列数据库</td><td>OpenTSDB</td><td>✔</td><td></td></tr><tr><td></td><td>TSDB</td><td>✔</td><td>✔</td></tr></tbody></table><h2 id="3-DataX使用"><a href="#3-DataX使用" class="headerlink" title="3.DataX使用"></a>3.DataX使用</h2><p>DataX的使用非常简单，用户仅需要根据自己同步数据的数据源和目的地的类型来选择相应的Reader和Writer插件即可，并将Reader和Writer插件的信息配置在一个json文件中，然后，在执行命令时，指定配置文件提交数据同步任务即可。</p><h3 id="3-1DataX配置文件格式"><a href="#3-1DataX配置文件格式" class="headerlink" title="3.1DataX配置文件格式"></a>3.1DataX配置文件格式</h3><p>这里以同步HDFS数据到MySQL为例，说明DataX的配置文件格式。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;job&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;setting&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;speed&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;channel&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;reader&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hdfsreader&quot;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/base_province&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;defaultFS&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hdfs://hadoop102:8020&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;column&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;*&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;fileType&quot;</span><span class="punctuation">:</span> <span class="string">&quot;text&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;compress&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gzip&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;encoding&quot;</span><span class="punctuation">:</span> <span class="string">&quot;UTF-8&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;nullFormat&quot;</span><span class="punctuation">:</span> <span class="string">&quot;\\N&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;fieldDelimiter&quot;</span><span class="punctuation">:</span> <span class="string">&quot;\t&quot;</span></span><br><span class="line">          <span class="punctuation">&#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;writer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">          <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mysqlwriter&quot;</span><span class="punctuation">,</span></span><br><span class="line">          <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;writeMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;replace&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;username&quot;</span><span class="punctuation">:</span> <span class="string">&quot;root&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;password&quot;</span><span class="punctuation">:</span> <span class="string">&quot;123456&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;column&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">              <span class="string">&quot;id&quot;</span><span class="punctuation">,</span></span><br><span class="line">              <span class="string">&quot;name&quot;</span><span class="punctuation">,</span></span><br><span class="line">              <span class="string">&quot;region_id&quot;</span><span class="punctuation">,</span></span><br><span class="line">              <span class="string">&quot;area_code&quot;</span><span class="punctuation">,</span></span><br><span class="line">              <span class="string">&quot;iso_code&quot;</span><span class="punctuation">,</span></span><br><span class="line">              <span class="string">&quot;iso_3166_2&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;connection&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">              <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;jdbcUrl&quot;</span><span class="punctuation">:</span> <span class="string">&quot;jdbc:mysql://hadoop102:3306/gmall?useUnicode=true&amp;characterEncoding=utf-8&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;table&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                  <span class="string">&quot;test_province&quot;</span></span><br><span class="line">                <span class="punctuation">]</span></span><br><span class="line">              <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">          <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>Reader和Writer的具体参数可参考[官方文档][<a href="https://github.com/alibaba/DataX/blob/master/README.md]">https://github.com/alibaba/DataX/blob/master/README.md]</a></p><h3 id="3-2DataX命令执行"><a href="#3-2DataX命令执行" class="headerlink" title="3.2DataX命令执行"></a>3.2DataX命令执行</h3><p>DataX的命令执行非常简单，只需要在命令行中执行如下命令：  </p><blockquote><p>python bin&#x2F;datax.py job&#x2F;base_province.json</p></blockquote><h5 id="3-2-1DataX传参"><a href="#3-2-1DataX传参" class="headerlink" title="3.2.1DataX传参"></a>3.2.1DataX传参</h5><p>在生产环境中，离线数据同步任务需要每日定时重复执行，故HDFS上的目标路径通常会包含一层日期，用来对每日同步的数据加以分区，也就是说每日同步数据的目标路径不是固定的，因此DataX配置文件中的DHFSWriter插件中参数path的值应该是动态变化的。为实现这个业务需求，我们需要使用DataX的传参功能<br><strong>DataX传参用法</strong><br>① 首选在任务的json配置文件中使用${param}引用参数<br>② 然后在提交任务时使用-p “-Dparam&#x3D;value” 传入参数值</p><h2 id="4-DataX调优"><a href="#4-DataX调优" class="headerlink" title="4.DataX调优"></a>4.DataX调优</h2><h3 id="4-1DataX运行流程"><a href="#4-1DataX运行流程" class="headerlink" title="4.1DataX运行流程"></a>4.1DataX运行流程</h3><p><img src="https://img01.zzh36111.us.kg/20250124214146.png"></p><h3 id="4-2DataX调度决策思路"><a href="#4-2DataX调度决策思路" class="headerlink" title="4.2DataX调度决策思路"></a>4.2DataX调度决策思路</h3><p>举例来说，用户提交了一个DataX作业，并且配置了总的并发度为20，目的是对一个有100张分表的mysql数据源进行同步。DataX的调度决策思路是：</p><ol><li>DataX Job根据分库分表切分策略(默认一个表一个Task，可以根据业务需要在reader插件中进行切分策略调整)，将同步工作分成100个Task。</li><li>DataX根据配置的总的并发度20，以及每个Task Group的并发度5，DataX计算共需要分配4个TaskGroup。</li><li>4个TaskGroup平分100个Task，每一个TaskGroup负责运行25个Task。</li></ol><h3 id="4-3DataX调优建议"><a href="#4-3DataX调优建议" class="headerlink" title="4.3DataX调优建议"></a>4.3DataX调优建议</h3><ol><li>速度控制:DataX中提供了包括通道（并发）、记录流、字节流三种流控模式，可以根据需要控制你的作业速度，让你的作业在数据库可以承受的范围内达到最佳的同步速度。</li><li>优化参数</li></ol><table><thead><tr><th>参数</th><th>说明</th><th>注意事项</th></tr></thead><tbody><tr><td><code>job.setting.speed.channel</code></td><td>并发数</td><td></td></tr><tr><td><code>job.setting.speed.record</code></td><td>总 record 限速</td><td>配置此参数，则必须配置单个 channel 的 record 限速参数</td></tr><tr><td><code>job.setting.speed.byte</code></td><td>总 byte 限速</td><td>配置此参数，则必须配置单个 channel 的 byte 限速参数</td></tr><tr><td><code>core.transport.channel.speed.record</code></td><td>单个 channel 的 record 限速，默认 10000 条&#x2F;s</td><td></td></tr><tr><td><code>core.transport.channel.speed.byte</code></td><td>单个 channel 的 byte 限速，默认 1M&#x2F;s</td><td></td></tr></tbody></table><p>注意：如果配置了总record限速和总byte限速，channel并发数就会失效。因为配置了这两个参数后，实际的channel并发数是通过计算得到的<br>①计算公式：<br>并发数&#x3D;min(byte_总⁄〖byte〗_单个channel ，  record_总⁄〖record〗_单个channel )<br>② 配置实例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;core&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;transport&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;channel&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;speed&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;byte&quot;</span><span class="punctuation">:</span> <span class="number">1048576</span> <span class="comment">//单个channel byte限速1M/s</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;job&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;setting&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;speed&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;byte&quot;</span> <span class="punctuation">:</span> <span class="number">5242880</span> <span class="comment">//总byte限速5M/s</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><!-- <div class="video-container">[up主专用，视频内嵌代码贴在这]</div> --><style>.video-container {    position: relative;    width: 100%;    padding-top: 56.25%; /* 16:9 aspect ratio (height/width = 9/16 * 100%) */}.video-container iframe {    position: absolute;    top: 0;    left: 0;    width: 100%;    height: 100%;}</style><!-- ---title: Datax<p>date: 1737698997000<br>tags:<br>— –&gt;</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>如何使用Maxwell进行全量表或增量表的数据同步</title>
      <link href="/wenzhang/Maxwell/"/>
      <url>/wenzhang/Maxwell/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Maxwell概述"><a href="#1-Maxwell概述" class="headerlink" title="1.Maxwell概述:"></a>1.Maxwell概述:</h2><p>Maxwell是使用Java编写(可以使用jps查看进程)的MySQL变更数据抓取软件。他会实时监控Mysql数据库的数据变更操作（包括insert、update、delete），并将变更数据以JSON的格式发送给Kafka、Kinesi等流数据处理平台。</p><h2 id="Maxwell输出数据格式"><a href="#Maxwell输出数据格式" class="headerlink" title="Maxwell输出数据格式"></a>Maxwell输出数据格式</h2><table><thead><tr><th>操作</th><th>SQL语句</th><th>数据格式</th></tr></thead><tbody><tr><td>插入</td><td><code>insert into gmall.student values(1, &#39;zhangsan&#39;);</code></td><td>&#96;&#96;&#96;json</td></tr><tr><td></td><td></td><td>{</td></tr><tr><td></td><td></td><td>“database”: “gmall”,</td></tr><tr><td></td><td></td><td>“table”: “student”,</td></tr><tr><td></td><td></td><td>“type”: “insert”,</td></tr><tr><td></td><td></td><td>“ts”: 1634004537,</td></tr><tr><td></td><td></td><td>“xid”: 1530907,</td></tr><tr><td></td><td></td><td>“commit”: true,</td></tr><tr><td></td><td></td><td>“data”: {</td></tr><tr><td></td><td></td><td>“id”: 1,</td></tr><tr><td></td><td></td><td>“name”: “zhangsan”</td></tr><tr><td></td><td></td><td>}</td></tr><tr><td></td><td></td><td>}</td></tr><tr><td></td><td></td><td>&#96;&#96;&#96;</td></tr><tr><td>更新</td><td><code>update gmall.student set name = &#39;lisi&#39; where id=1;</code></td><td>&#96;&#96;&#96;json</td></tr><tr><td></td><td></td><td>{</td></tr><tr><td></td><td></td><td>“database”: “gmall”,</td></tr><tr><td></td><td></td><td>“table”: “student”,</td></tr><tr><td></td><td></td><td>“type”: “update”,</td></tr><tr><td></td><td></td><td>“ts”: 1634004657,</td></tr><tr><td></td><td></td><td>“xid”: 1531916,</td></tr><tr><td></td><td></td><td>“commit”: true,</td></tr><tr><td></td><td></td><td>“data”: {</td></tr><tr><td></td><td></td><td>“id”: 1,</td></tr><tr><td></td><td></td><td>“name”: “lisi”</td></tr><tr><td></td><td></td><td>},</td></tr><tr><td></td><td></td><td>“old”: {</td></tr><tr><td></td><td></td><td>“name”: “zhangsan”</td></tr><tr><td></td><td></td><td>}</td></tr><tr><td></td><td></td><td>}</td></tr><tr><td></td><td></td><td>&#96;&#96;&#96;</td></tr><tr><td>删除</td><td><code>delete from gmall.student where id=1;</code></td><td>&#96;&#96;&#96;json</td></tr><tr><td></td><td></td><td>{</td></tr><tr><td></td><td></td><td>“database”: “gmall”,</td></tr><tr><td></td><td></td><td>“table”: “student”,</td></tr><tr><td></td><td></td><td>“type”: “delete”,</td></tr><tr><td></td><td></td><td>“ts”: 1634004751,</td></tr><tr><td></td><td></td><td>“xid”: 1532725,</td></tr><tr><td></td><td></td><td>“commit”: true,</td></tr><tr><td></td><td></td><td>“data”: {</td></tr><tr><td></td><td></td><td>“id”: 1,</td></tr><tr><td></td><td></td><td>“name”: “lisi”</td></tr><tr><td></td><td></td><td>}</td></tr><tr><td></td><td></td><td>}</td></tr><tr><td></td><td></td><td>&#96;&#96;&#96;</td></tr></tbody></table><p><strong>字段说明：</strong></p><ol><li><code>database</code>：变更数据所属的数据库</li><li><code>commit</code>：事务提交标志，可用于重新组装事务。 </li><li>xid：事务ID，用于标识事务。 </li><li>ts：变更数据的时间戳。 </li><li>type：变更数据的类型，包括insert、update、delete。 </li><li>data：变更数据，包括新增或修改的数据。 </li><li>old：变更前的数据，仅在update操作时存在。</li></ol><h2 id="2-Maxwell的实现原理"><a href="#2-Maxwell的实现原理" class="headerlink" title="2.Maxwell的实现原理"></a>2.Maxwell的实现原理</h2><p>Maxwell的实现原理很简单，就是将自己伪装成Slave，并遵循Mysql主从复制的协议，从master中同步数据。<br><strong>Mysql主从复制工作原理</strong><img src="https://img01.zzh36111.us.kg/20250123182703.png"><br>① Master主库接收到数据变更请求，完成数据变更，并将其写到二级制日志（binary log）中。<br>② Slave从库向Mysql master发送dump协议，将Master主库的binary log events拷贝到从库的中继日志（relay log）中。<br>③ Slave从库读取并回放中继日志中的事件，将更新的数据同步到自己的数据库中。</p><h2 id="3-Maxwell部署"><a href="#3-Maxwell部署" class="headerlink" title="3.Maxwell部署"></a>3.Maxwell部署</h2><h3 id="1-配置MySQL"><a href="#1-配置MySQL" class="headerlink" title="1.配置MySQL"></a>1.配置MySQL</h3><h4 id="1-1"><a href="#1-1" class="headerlink" title="1.1"></a>1.1</h4><p>配置MySQL启动MySQL Binlog（修改MySQL的配置文件&#x2F;etc&#x2F;my.cnf重启Mysql服务后才生效）</p><h4 id="2-1"><a href="#2-1" class="headerlink" title="2.1"></a>2.1</h4><p>Mysql Binlog模式介绍<br>① Statement-based: 基于语句，Binlog会记录所有写操作的SQL语句，包括：insert、update、delete等。<br>                    优点：节省空间；<br>                    缺点：有可能造成数据不一致，如：insert语句中包含now()函数。<br>② Row-based：       基于行，Binlog会记录每次写操作的被操作行记录的变化。<br>                    优点：保持数据的绝对一致性<br>                    缺点：占用较大空间<br>③ mixed：   混合模式，默认是Statement-based，如果SQL语句可能导致数据不一致，就自动切换到Row-based<br>Maxwell要求Binlog采用Row-based模式</p><h3 id="2-创建Maxwell所需数据库和用户"><a href="#2-创建Maxwell所需数据库和用户" class="headerlink" title="2.创建Maxwell所需数据库和用户"></a>2.创建Maxwell所需数据库和用户</h3><h3 id="3-配置Maxwell"><a href="#3-配置Maxwell" class="headerlink" title="3.配置Maxwell"></a>3.配置Maxwell</h3><h4 id="3-1"><a href="#3-1" class="headerlink" title="3.1"></a>3.1</h4><p>修改Maxwell的配置文件config.properties,主要是配置数据发送的目的地。（若Maxwell发送数据的目的地是kafka集群，需要首先将kafka集群启动。）</p><h2 id="4-启动Maxwell"><a href="#4-启动Maxwell" class="headerlink" title="4.启动Maxwell"></a>4.启动Maxwell</h2><p>Maxwell的启动命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/maxwell --config config.properties --daemon</span><br></pre></td></tr></table></figure><p>Maxwell关闭命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep maxwell | grep -v grep | grep maxwell | awk &#x27;&#123;print $2&#125;&#x27; | xargs kill -9</span><br></pre></td></tr></table></figure><h3 id="Maxwell全量数据同步流程命令"><a href="#Maxwell全量数据同步流程命令" class="headerlink" title="Maxwell全量数据同步流程命令"></a>Maxwell全量数据同步流程命令</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/maxwell-bootstrap --database edu --table user_info --config config.properties</span><br></pre></td></tr></table></figure><!-- <div class="video-container">[up主专用，视频内嵌代码贴在这]</div> --><style>.video-container {    position: relative;    width: 100%;    padding-top: 56.25%; /* 16:9 aspect ratio (height/width = 9/16 * 100%) */}.video-container iframe {    position: absolute;    top: 0;    left: 0;    width: 100%;    height: 100%;}</style><!-- ---title: 如何使用Maxwell进行全量表或增量表的数据同步<p>date: 1737624278000<br>tags:<br>— –&gt;</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>在线教育离线数仓之数据采集</title>
      <link href="/wenzhang/%E5%9C%A8%E7%BA%BF%E6%95%99%E8%82%B2%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"/>
      <url>/wenzhang/%E5%9C%A8%E7%BA%BF%E6%95%99%E8%82%B2%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<!-- <div class="video-container">[up主专用，视频内嵌代码贴在这]</div> --><style>.video-container {    position: relative;    width: 100%;    padding-top: 56.25%; /* 16:9 aspect ratio (height/width = 9/16 * 100%) */}.video-container iframe {    position: absolute;    top: 0;    left: 0;    width: 100%;    height: 100%;}</style><!-- ---title: 在线教育离线数仓之数据采集<p>date: 1737542912000<br>tags:<br>— –&gt;</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/wenzhang/hello-world/"/>
      <url>/wenzhang/hello-world/</url>
      
        <content type="html"><![CDATA[<h1 id="博客预览"><a href="#博客预览" class="headerlink" title="博客预览"></a>博客预览</h1><blockquote><p>hexo cl; hexo s</p></blockquote><h1 id="推送"><a href="#推送" class="headerlink" title="推送"></a>推送</h1><blockquote><p>hexo cl; hexo g; hexo d</p></blockquote><h3 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h3><h2 id="基本语法-1"><a href="#基本语法-1" class="headerlink" title="基本语法"></a>基本语法</h2><h3 id="基本语法-2"><a href="#基本语法-2" class="headerlink" title="基本语法"></a>基本语法</h3><p><strong>引用</strong></p><ol><li>hjjk </li><li>jkdsfjl</li></ol><ul><li>skolfakashjd </li><li>kdsfjfjljds<br>djlksgjdfjksldfjl</li></ul><ul><li>sakllflj</li></ul><ul><li>sakhaklfhk</li></ul><ol><li>lsalk</li></ol><table><thead><tr><th>表头 1</th><th>表头 2</th></tr></thead><tbody><tr><td>内容 1</td><td>内容 2</td></tr><tr><td>内容 3</td><td>内容 4</td></tr></tbody></table><div style="text-align: center;"><table><thead><tr><th align="right">kalfl</th><th align="center">ldskkfldg</th></tr></thead><tbody><tr><td align="right">lslfjkk</td><td align="center">kdjfglk</td></tr></tbody></table></div><div style="text-align: center;"><table><thead><tr><th>服务名称</th><th>子服务</th><th>hadoop102</th><th>hadoop103</th><th>hadoop104</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode</td><td>√</td><td></td><td></td></tr><tr><td>HDFS</td><td>DataNode</td><td>√</td><td>√</td><td>√</td></tr><tr><td>HDFS</td><td>SecondaryNameNode</td><td></td><td></td><td>√</td></tr><tr><td>Yarn</td><td>NodeManager</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Yarn</td><td>Resourcemanager</td><td></td><td>√</td><td></td></tr><tr><td>Zookeeper</td><td>Zookeeper Server</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Flume(采集日志)</td><td>Flume</td><td>√</td><td>√</td><td></td></tr><tr><td>Kafka</td><td>Kafka</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Flume（消费Kafka日志）</td><td>Flume</td><td></td><td></td><td>√</td></tr><tr><td>Flume（消费Kafka业务）</td><td>Flume</td><td></td><td></td><td>√</td></tr><tr><td>Hive</td><td>Hive</td><td>√</td><td></td><td></td></tr><tr><td>MySQL</td><td>MySQL</td><td>√</td><td></td><td></td></tr><tr><td>DataX</td><td>DataX</td><td>√</td><td></td><td></td></tr><tr><td>Maxwell</td><td>Maxwell</td><td>√</td><td></td><td></td></tr><tr><td>Spark</td><td></td><td>√</td><td>√</td><td>√</td></tr><tr><td>ClickHouse</td><td></td><td>√</td><td></td><td></td></tr><tr><td>DolphinScheduler</td><td>ApiApplicationServer</td><td>√</td><td></td><td></td></tr><tr><td>DolphinScheduler</td><td>AlertServer</td><td>√</td><td></td><td></td></tr><tr><td>DolphinScheduler</td><td>MasterServer</td><td>√</td><td></td><td></td></tr><tr><td>DolphinScheduler</td><td>WorkerServer</td><td>√</td><td>√</td><td>√</td></tr><tr><td>DolphinScheduler</td><td>LoggerServer</td><td>√</td><td>√</td><td>√</td></tr></tbody></table></div>`dfhjkgh`<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lhjsdfkhs</span><br><span class="line">jkhdsfkj</span><br></pre></td></tr></table></figure><p>alt+shift+f格式化表格</p><p><a href="https://cmliussss.com/p/HexoBlogNo2/">超链接</a>：<a href="https://cmliussss.com/p/HexoBlogNo2/">https://cmliussss.com/p/HexoBlogNo2/</a></p><p>请<a href="https://cmliussss.com/p/HexoBlogNo2/">点我</a></p><h1 id="服务部署情况"><a href="#服务部署情况" class="headerlink" title="服务部署情况"></a>服务部署情况</h1><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>在线教育离线数仓使用的脚本合集</title>
      <link href="/wenzhang/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E9%A1%B9%E7%9B%AE/"/>
      <url>/wenzhang/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E9%A1%B9%E7%9B%AE/</url>
      
        <content type="html"><![CDATA[<h3 id="1-集群执行命令脚本："><a href="#1-集群执行命令脚本：" class="headerlink" title="1. 集群执行命令脚本："></a>1. 集群执行命令脚本：</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">! /bin/bash</span></span><br><span class="line"> </span><br><span class="line">for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">    echo --------- $i ----------</span><br><span class="line">    ssh $i &quot;$*&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h3 id="2-集群同步脚本"><a href="#2-集群同步脚本" class="headerlink" title="2. 集群同步脚本"></a>2. 集群同步脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">1. 判断参数个数</span></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">  echo Not Enough Arguement!</span><br><span class="line">  exit;</span><br><span class="line">fi</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">2. 遍历集群所有机器</span></span><br><span class="line">for host in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">  echo ====================  $host  ====================</span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">3. 遍历所有目录，挨个发送</span></span><br><span class="line">  for file in $@</span><br><span class="line">  do</span><br><span class="line">    #4 判断文件是否存在</span><br><span class="line">    if [ -e $file ]</span><br><span class="line">    then</span><br><span class="line">      #5. 获取父目录(dirname获取绝对路径父目录,相对路径获取的是.）</span><br><span class="line">      pdir=$(cd -P $(dirname $file); pwd)</span><br><span class="line">      #6. 获取当前文件的名称</span><br><span class="line">      fname=$(basename $file)</span><br><span class="line">      ssh $host &quot;mkdir -p $pdir&quot;</span><br><span class="line">      rsync -av $pdir/$fname $host:$pdir #-av是打印进度的</span><br><span class="line">    else</span><br><span class="line">      echo $file does not exists!</span><br><span class="line">    fi</span><br><span class="line">  done</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h3 id="3-Hadoop启动脚本"><a href="#3-Hadoop启动脚本" class="headerlink" title="3. Hadoop启动脚本"></a>3. Hadoop启动脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; =================== 启动 hadoop集群 ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot; --------------- 启动 hdfs ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop/sbin/start-dfs.sh&quot;</span><br><span class="line">        echo &quot; --------------- 启动 yarn ---------------&quot;</span><br><span class="line">        ssh hadoop103 &quot;/opt/module/hadoop/sbin/start-yarn.sh&quot;</span><br><span class="line">        echo &quot; --------------- 启动 historyserver ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop/bin/mapred --daemon start historyserver&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; =================== 关闭 hadoop集群 ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot; --------------- 关闭 historyserver ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop/bin/mapred --daemon stop historyserver&quot;</span><br><span class="line">        echo &quot; --------------- 关闭 yarn ---------------&quot;</span><br><span class="line">        ssh hadoop103 &quot;/opt/module/hadoop/sbin/stop-yarn.sh&quot;</span><br><span class="line">        echo &quot; --------------- 关闭 hdfs ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop/sbin/stop-dfs.sh&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h3 id="4-Zookeeper启动脚本"><a href="#4-Zookeeper启动脚本" class="headerlink" title="4. Zookeeper启动脚本"></a>4. Zookeeper启动脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">判断是否输入参数</span></span><br><span class="line">if [ $# -lt 1 ]; then</span><br><span class="line">  echo &quot;请输入参数&quot;</span><br><span class="line">  exit</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">        for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">        do</span><br><span class="line">        echo ---------- zookeeper $i 启动 ------------</span><br><span class="line">                ssh $i &quot;/opt/module/zookeeper/bin/zkServer.sh start&quot;</span><br><span class="line">        done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">        for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">        do</span><br><span class="line">        echo ---------- zookeeper $i 停止 ------------    </span><br><span class="line">                ssh $i &quot;/opt/module/zookeeper/bin/zkServer.sh stop&quot;</span><br><span class="line">        done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;status&quot;)&#123;</span><br><span class="line">        for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">        do</span><br><span class="line">        echo ---------- zookeeper $i 状态 ------------    </span><br><span class="line">                ssh $i &quot;/opt/module/zookeeper/bin/zkServer.sh status&quot;</span><br><span class="line">        done</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h3 id="5-Kafka启动脚本"><a href="#5-Kafka启动脚本" class="headerlink" title="5. Kafka启动脚本"></a>5. Kafka启动脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">! /bin/bash</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">判断是否输入参数</span></span><br><span class="line">if [ $# -lt 1 ]; then</span><br><span class="line">  echo &quot;请输入参数&quot;</span><br><span class="line">  exit</span><br><span class="line">fi</span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">    for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">    do</span><br><span class="line">        echo &quot; --------启动 $i Kafka-------&quot;</span><br><span class="line">        ssh $i &quot;/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties&quot;</span><br><span class="line">    done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">    for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">    do</span><br><span class="line">        echo &quot; --------停止 $i Kafka-------&quot;</span><br><span class="line">        ssh $i &quot;/opt/module/kafka/bin/kafka-server-stop.sh &quot;</span><br><span class="line">    done</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p>maxwell启动脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">MAXWELL_HOME=/opt/module/maxwell</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">grep -v grep是为了过滤掉grep本身的进程</span></span><br><span class="line">result=$(ps -ef | grep Maxwell | grep -v grep)</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">start )</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">判断result是否为空</span></span><br><span class="line">    if [ -z &quot;$result&quot; ]; then</span><br><span class="line">        echo &quot;启动Maxwell&quot;</span><br><span class="line">        $MAXWELL_HOME/bin/maxwell --config $MAXWELL_HOME/config.properties --daemon</span><br><span class="line">    else</span><br><span class="line">         echo &quot;Maxwell正在运行&quot;</span><br><span class="line">    fi</span><br><span class="line">;;</span><br><span class="line">stop )</span><br><span class="line">    if [ -z &quot;$result&quot; ]; then</span><br><span class="line">        echo &quot;Maxwell未在运行&quot;</span><br><span class="line">    else</span><br><span class="line">echo &quot;停止Maxwell&quot;</span><br><span class="line">        ps -ef | grep Maxwell | grep -v grep | awk &#x27;&#123;print $2&#125;&#x27; | xargs kill -9</span><br><span class="line">    fi</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><!-- <div class="video-container">[up主专用，视频内嵌代码贴在这]</div> --><style>.video-container {    position: relative;    width: 100%;    padding-top: 56.25%; /* 16:9 aspect ratio (height/width = 9/16 * 100%) */}.video-container iframe {    position: absolute;    top: 0;    left: 0;    width: 100%;    height: 100%;}</style>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>大数据项目之在线教育离线数仓</title>
      <link href="/wenzhang/%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E6%96%B0%E7%9A%84%E5%8D%9A%E6%96%87/"/>
      <url>/wenzhang/%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E6%96%B0%E7%9A%84%E5%8D%9A%E6%96%87/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据开发项目之在线教育离线数仓从零到一（学习项目）"><a href="#大数据开发项目之在线教育离线数仓从零到一（学习项目）" class="headerlink" title="大数据开发项目之在线教育离线数仓从零到一（学习项目）"></a>大数据开发项目之在线教育离线数仓从零到一（学习项目）</h1><p>本文详细介绍了大数据开发项目之在线教育离线数仓的开发过程，从零到一，包括项目背景、需求分析、设计思路、技术选型、开发环境搭建、数据采集、数仓搭建、</p><h2 id="项目整体流程"><a href="#项目整体流程" class="headerlink" title="项目整体流程"></a>项目整体流程</h2><p>该项目是一个在线教育大数据开发项目，旨在构建离线数仓，对在线教育业务数据进行分析处理，以支持企业决策。以下是项目的详细流程：</p><h3 id="1-业务流程与数据来源"><a href="#1-业务流程与数据来源" class="headerlink" title="1. 业务流程与数据来源"></a>1. 业务流程与数据来源</h3><ul><li><strong>业务流程</strong>：用户从在线教育网站首页开始浏览课程，可通过分类查询或全文检索寻找课程，找到后可添加到购物车、登录、结算，生成订单和支付数据，订单生成后会进行跟踪处理。</li><li><strong>数据来源</strong>：包括用户行为数据（通过前端埋点采集，存储在 HDFS 文件中）和业务数据（存储在 MySQL 中）。(因为本项目为学习项目，所以数据来源均为数据模拟器模拟的虚拟数据)</li></ul><h3 id="2-系统数据流程设计"><a href="#2-系统数据流程设计" class="headerlink" title="2. 系统数据流程设计"></a>2. 系统数据流程设计</h3><ul><li><strong>集群流程图</strong>：业务服务器与 App 业务交互，通过 Nginx 进行数据传输。日志数据经日志服务器采集，可采用 flume 采集方式，部分数据经消息缓存后存入 Kafka。业务数据通过 DataX 每日同步从 MySQL 数据库导入。数据在集群中经过 ODS、DWD、DWS 等层的处理，最终用于可视化。<br><img src="https://img01.zzh36111.us.kg/20250122151413.png" alt="流程图"><br>问：为什么不直接使用flume把数据采集到hdfs，要在之间加一个kafka呢？可以把Kafka换为flume聚合吗？</li><li><strong>集群特点</strong>：具备多数据源对接能力，可进行离线批量和在线实时处理，有统一的集群管理配置监控平台，并实现用户认证和权限管理，满足多租户需求。</li></ul><h3 id="3-技术选型"><a href="#3-技术选型" class="headerlink" title="3. 技术选型"></a>3. 技术选型</h3><ul><li><p><strong>Apache 框架版本</strong>：确定了 Hadoop、Flume、Kafka、Hive、Sqoop 等多种技术框架的具体版本，每个版本都有其特定的功能特点，以满足项目需求。</p><table><thead><tr><th>框架</th><th>版本</th></tr></thead><tbody><tr><td>Hadoop</td><td>3.1.3</td></tr><tr><td>Zookeeper</td><td>3.5.7</td></tr><tr><td>MySQL</td><td>5.7.16</td></tr><tr><td>Hive</td><td>3.1.2</td></tr><tr><td>Flume</td><td>1.9.0</td></tr><tr><td>Kafka</td><td>3.0.0</td></tr><tr><td>Spark</td><td>3.0.0</td></tr><tr><td>DataX</td><td>3.0.0</td></tr><tr><td>Superset</td><td>1.3.2</td></tr><tr><td>DolphinScheduler</td><td>2.0.3</td></tr><tr><td>Maxwell</td><td>1.29.2</td></tr></tbody></table></li><li><p><strong>服务器选型</strong>：因为本项目仅为学习项目所以服务器选型比较简单仅为三台虚拟机。</p></li></ul><h3 id="4-集群规模规划"><a href="#4-集群规模规划" class="headerlink" title="4. 集群规模规划"></a>4. 集群规模规划</h3><ul><li><p><strong>集群规划</strong>：详细规划了各服务器节点上部署的组件，如 DataNode、NodeManager、ResourceManager 等在不同服务器上的分布。</p><table><thead><tr><th>服务名称</th><th>子服务</th><th>hadoop102</th><th>hadoop103</th><th>hadoop104</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode</td><td>√</td><td></td><td></td></tr><tr><td>HDFS</td><td>DataNode</td><td>√</td><td>√</td><td>√</td></tr><tr><td>HDFS</td><td>SecondaryNameNode</td><td></td><td></td><td>√</td></tr><tr><td>Yarn</td><td>NodeManager</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Yarn</td><td>Resourcemanager</td><td></td><td>√</td><td></td></tr><tr><td>Zookeeper</td><td>Zookeeper Server</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Flume(采集日志)</td><td>Flume</td><td>√</td><td>√</td><td></td></tr><tr><td>Kafka</td><td>Kafka</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Flume（消费Kafka日志）</td><td>Flume</td><td></td><td></td><td>√</td></tr><tr><td>Flume（消费Kafka业务）</td><td>Flume</td><td></td><td></td><td>√</td></tr><tr><td>Hive</td><td>Hive</td><td>√</td><td></td><td></td></tr><tr><td>MySQL</td><td>MySQL</td><td>√</td><td></td><td></td></tr><tr><td>DataX</td><td>DataX</td><td>√</td><td></td><td></td></tr><tr><td>Maxwell</td><td>Maxwell</td><td>√</td><td></td><td></td></tr><tr><td>Spark</td><td></td><td>√</td><td>√</td><td>√</td></tr><tr><td>DolphinScheduler</td><td>ApiApplicationServer</td><td>√</td><td></td><td></td></tr><tr><td>DolphinScheduler</td><td>AlertServer</td><td>√</td><td></td><td></td></tr><tr><td>DolphinScheduler</td><td>MasterServer</td><td>√</td><td></td><td></td></tr><tr><td>DolphinScheduler</td><td>WorkerServer</td><td>√</td><td>√</td><td>√</td></tr><tr><td>DolphinScheduler</td><td>LoggerServer</td><td>√</td><td>√</td><td>√</td></tr></tbody></table></li></ul><!-- ### 5. 数据生成与分析指标 - **数据生成器使用**：通过上传相关配置文件（application.yml、path.json、logback.xml 等）到指定目录，并修改配置文件参数，可在虚拟机指定路径下生成日志数据并向业务数据库插入业务数据。可生成不同日期数据，且能灵活配置用户点击路径和日志生成路径。 - **分析指标**：涵盖流量主题（如各来源流量统计、路径分析、各来源下单统计）、用户主题（如用户变动统计、留存率、新增活跃统计等）、课程主题（如各分类课程交易统计、评价统计、试听留存统计等）、交易主题、考试主题、播放主题、完课主题等多方面的指标，每个指标都有明确的统计周期和粒度要求。 -->]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
